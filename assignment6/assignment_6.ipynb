{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "  <img src=\"https://th.bing.com/th/id/R.3cd1c8dc996c5616cf6e65e20b6bf586?rik=09aaLyk4hfbBiQ&riu=http%3a%2f%2fcidics.uanl.mx%2fwp-content%2fuploads%2f2016%2f09%2fcimat.png&ehk=%2b0brgMUkA2BND22ixwLZheQrrOoYLO3o5cMRqsBOrlY%3d&risl=&pid=ImgRaw&r=0\" \n",
    "     style=\"float: right; margin-right: 30px;\" \n",
    "     width=\"120\"\n",
    "     />\n",
    "\n",
    " ---\n",
    " \n",
    " # **OPTIMIZACIÓN: TAREA 6**\n",
    " EZAU FARIDH TORRES TORRES.\n",
    "     \n",
    "<p align=\"right\"> Maestría en Ciencias con Orientación en Matemáticas Aplicadas. </p>\n",
    "<p align=\"right\"> CENTRO DE INVESTIGACIÓN EN MATEMÁTICAS. </p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA: Se usa sólo la condición débil de Wolf ya que la fuerte no convergió en ningún caso para los parámetros dados.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = \"dark\")\n",
    "\n",
    "def BACKTRAKING_WOLF(a_init: float, p: float, c1: float,\n",
    "                c2: float, xk: np.array, f, gradf,\n",
    "                dk: np.array, Nb: int):\n",
    "    a = a_init\n",
    "    for i in range(Nb): # STRONG / NORMAL\n",
    "        #if (f(xk + a*dk) <= f(xk) + c1*a*gradf(xk).T @ dk) and (np.abs(gradf(xk + a*dk).T @ dk) <= -c2*gradf(xk).T @ dk):\n",
    "        if (f(xk + a*dk) <= f(xk) + c1*a*gradf(xk).T @ dk) and (gradf(xk + a*dk).T @ dk >= c2*gradf(xk).T @ dk):\n",
    "            return a, i\n",
    "        a = p*a\n",
    "    return a, Nb\n",
    "\n",
    "def f_Himmelblau(x: np.array):\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "def grad_Himmelblau(x: np.array):\n",
    "    x1 = 4*x[0]*(x[0]**2 + x[1] - 11) + 2*(x[0] + x[1]**2 - 7)\n",
    "    x2 = 2*(x[0]**2 + x[1] - 11) + 4*x[1]*(x[0] + x[1]**2 - 7)\n",
    "    return np.array([x1,x2], dtype = float)\n",
    "\n",
    "def f_Beale(x: np.array):\n",
    "    return (1.5 - x[0] + x[0]*x[1])**2 + (2.25 - x[0] + x[0]*x[1]**2)**2 + (2.625 - x[0] + x[0]*x[1]**3)**2\n",
    "def grad_Beale(x: np.array):\n",
    "    x1 = 2*(x[1] - 1)*(1.5 - x[0] + x[0]*x[1]) + 2*(x[1]**2 - 1)*(2.25 - x[0] + x[0]*x[1]**2) + 2*(x[1]**3 - 1)*(2.625 - x[0] + x[0]*x[1]**3)\n",
    "    x2 = 2*x[0]*(1.5 - x[0] + x[0]*x[1]) + 4*x[0]*x[1]*(2.25 - x[0] + x[0]*x[1]**2) + 6*x[0]*(x[1]**2)*(2.625 - x[0] + x[0]*x[1]**3)\n",
    "    return np.array([x1,x2], dtype = float)\n",
    "\n",
    "def f_Rosenbrock(x: np.array):\n",
    "    n = len(x)\n",
    "    s = 0\n",
    "    for i in range(n-1):\n",
    "        s = s + 100*(x[i+1] - x[i]**2)**2 + (1 - x[i])**2\n",
    "    return s\n",
    "def grad_Rosenbrock(x: np.array):\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    grad[0] = -400*x[0]*(x[1] - x[0]**2) - 2*(1-x[0])\n",
    "    grad[n-1] = 200*(x[n-1] - x[n-2]**2)\n",
    "    for j in range(1,n-1):\n",
    "        grad[j] = 200*(x[j]-x[j-1]**2) - 400*x[j]*(x[j+1] - x[j]**2) - 2*(1-x[j])\n",
    "    return np.array(grad, dtype = float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **1.- Ejercicio 1:**\n",
    "\n",
    "---\n",
    "\n",
    "## **1.1.**\n",
    "\n",
    "Programe el método de gradiente conjugado lineal, Algoritmo 1 de la Clase 18, para resolver el sistema de ecuaciones $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$, donde $\\mathbf{A}$ es una matriz simétrica y definida positiva. Haga que la función devuelva el último punto  $\\mathbf{x}_k$, el último residual $\\mathbf{r}_k$, el número de iteraciones $k$ y una variable binaria $bres$ que indique si se cumplió el criterio de paro ($bres=True$) o si el algoritmo terminó por iteraciones ($bres=False$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LINEAR_CONJUGATE_GRADIENT(xk: np.array, A: np.array,\n",
    "                        b: np.array, maxiter: int, tol: float):\n",
    "    \"\"\"\n",
    "    SOLVE THE SYSTEM OF EQUATIONS Ax = b, WHERE A IS A POSITIVE DEFINITE SYMMETRIC MATRIX USING THE CONJUGATE GRADIENT METHOD\n",
    "\n",
    "    Args:\n",
    "    - xk:      initial guess.\n",
    "    - A:       positive definite symmetric matrix.\n",
    "    - b:       vector b.\n",
    "    - maxiter: maximum number of iterations.\n",
    "    - tol:     tolerance.\n",
    "\n",
    "    Outputs:\n",
    "    - xk:  approach to the solution.\n",
    "    - rk:  residual.\n",
    "    - k:   number of iterations.\n",
    "    - T/F: if the method converged.\n",
    "    \"\"\"\n",
    "    rk = A @ xk - b\n",
    "    pk = - rk\n",
    "    for k in range(maxiter+1):\n",
    "        if np.linalg.norm(rk) < tol:\n",
    "            return xk, rk, k, True\n",
    "        ak = (rk.T @ rk) / (pk.T @ A @ pk)\n",
    "        xk = xk + ak * pk\n",
    "        rk_new = rk + ak * A @ pk\n",
    "        bk = (rk_new.T @ rk_new) / (rk.T @ rk)\n",
    "        pk = - rk_new + bk * pk\n",
    "        rk = rk_new\n",
    "    return xk, rk, maxiter, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2.**\n",
    "\n",
    "Pruebe el algoritmo para resolver el sistema de ecuaciones \n",
    "\n",
    "$$ \\mathbf{A}_1\\mathbf{x}=\\mathbf{b}_1$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$ \\mathbf{A}_1 = n\\mathbf{I} + \\mathbf{1} = \n",
    "\\left[\\begin{array}{llll} n      & 0      & \\cdots & 0 \\\\\n",
    "                       0      & n      & \\cdots & 0 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       0      & 0      & \\cdots & n \\end{array}\\right]\n",
    "+ \\left[\\begin{array}{llll} 1    & 1      & \\cdots & 1 \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\end{array}\\right],  \\qquad\n",
    "\\mathbf{b}_1 = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right], $$\n",
    "\n",
    "$n$ es la dimensión de la variable independiente\n",
    "$\\mathbf{x}=(x_1, x_2, ..., x_n)$, \n",
    "$\\mathbf{I}$ es la matriz identidad y $\\mathbf{1}$ es la matriz llena de 1's,\n",
    "ambas de tamaño $n$.\n",
    "\n",
    "- Use $\\mathbf{x}_0$ como el vector cero, el máximo número de iteraciones \n",
    "  $N=n$ y una toleracia $\\tau=\\sqrt{n} \\epsilon_m^{1/3}$,\n",
    "  donde $\\epsilon_m$ es el épsilon máquina.\n",
    "- Pruebe el algoritmo resolviendo los dos sistemas de ecuaciones con $n=10, 100, 1000$ y \n",
    "  en cada caso imprima la siguiente información\n",
    "\n",
    "- la dimensión $n$,\n",
    "- el  número $k$ de iteraciones realizadas,\n",
    "- las primeras y últimas 4 entradas del punto $\\mathbf{x}_k$ que devuelve el algoritmo,\n",
    "- la norma del residual $\\mathbf{r}_k$, \n",
    "- la variable $bres$ para saber si el algoritmo puedo converger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    10\n",
      "ITERACIONES:  1\n",
      "xk:           [0.05 0.05 0.05 0.05] ... [0.05 0.05 0.05 0.05]\n",
      "NORMA rk:     6.280369834735101e-16\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "xk, rk, k, conv = LINEAR_CONJUGATE_GRADIENT(xk = x0, A = A1,\n",
    "                            b = b1, maxiter = n, tol = tol)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA rk:    \", np.linalg.norm(rk))\n",
    "print(\"CONVERGENCIA:\", conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    100\n",
      "ITERACIONES:  1\n",
      "xk:           [0.005 0.005 0.005 0.005] ... [0.005 0.005 0.005 0.005]\n",
      "NORMA rk:     7.691850745534255e-16\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "xk, rk, k, conv = LINEAR_CONJUGATE_GRADIENT(xk = x0, A = A1,\n",
    "                            b = b1, maxiter = n, tol = tol)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA rk:    \", np.linalg.norm(rk))\n",
    "print(\"CONVERGENCIA:\", conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    1000\n",
      "ITERACIONES:  1\n",
      "xk:           [0.0005 0.0005 0.0005 0.0005] ... [0.0005 0.0005 0.0005 0.0005]\n",
      "NORMA rk:     1.192021805172505e-13\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "xk, rk, k, conv = LINEAR_CONJUGATE_GRADIENT(xk = x0, A = A1,\n",
    "                            b = b1, maxiter = n, tol = tol)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA rk:    \", np.linalg.norm(rk))\n",
    "print(\"CONVERGENCIA:\", conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3.**\n",
    "\n",
    "También aplique el algoritmo para resolver el sistema \n",
    "\n",
    "$$ \\mathbf{A}_2\\mathbf{x}=\\mathbf{b}_2$$\n",
    "\n",
    "donde  $\\mathbf{A}_2 = [a_{ij}]$ con\n",
    "\n",
    "$$ a_{ij} = exp\\left(-0.25(i-j)^2 \\right),  \\qquad\n",
    "\\mathbf{b}_2 = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    10\n",
      "ITERACIONES:  5\n",
      "xk:           [ 1.36909916 -1.16637682  1.60908281 -0.61339053] ... [-0.61339053  1.60908281 -1.16637682  1.36909916]\n",
      "NORMA rk:     3.5332157369569496e-12\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "xk, rk, k, conv = LINEAR_CONJUGATE_GRADIENT(xk = x0, A = A2,\n",
    "                            b = b2, maxiter = n, tol = tol)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA rk:    \", np.linalg.norm(rk))\n",
    "print(\"CONVERGENCIA:\", conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    100\n",
      "ITERACIONES:  100\n",
      "xk:           [ 1.44610292 -1.41613796  2.11052474 -1.42522358] ... [-1.42492099  2.11046082 -1.41638734  1.44632425]\n",
      "NORMA rk:     0.0002433600691892639\n",
      "CONVERGENCIA: False\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "xk, rk, k, conv = LINEAR_CONJUGATE_GRADIENT(xk = x0, A = A2,\n",
    "                            b = b2, maxiter = n, tol = tol)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA rk:    \", np.linalg.norm(rk))\n",
    "print(\"CONVERGENCIA:\", conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    1000\n",
      "ITERACIONES:  262\n",
      "xk:           [ 1.44628824 -1.41635954  2.1105181  -1.42507231] ... [-1.42507231  2.1105181  -1.41635954  1.44628824]\n",
      "NORMA rk:     0.00018766135449068362\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "xk, rk, k, conv = LINEAR_CONJUGATE_GRADIENT(xk = x0, A = A2,\n",
    "                            b = b2, maxiter = n, tol = tol)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA rk:    \", np.linalg.norm(rk))\n",
    "print(\"CONVERGENCIA:\", conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **2.- Ejercicio 2:**\n",
    "\n",
    "---\n",
    "\n",
    "Programar el método de gradiente conjugado no lineal descrito en el Algoritmo 3 de Clase 19 usando la fórmula de Fletcher-Reeves:\n",
    "\n",
    "$$ \\beta_{k+1} = \\frac{\\nabla f_{k+1}^\\top \\nabla f_{k+1}}{\\nabla f_{k}^\\top\\nabla f_{k}}  $$ \n",
    "\n",
    "## **2.1.**\n",
    "Escriba la función que implemente el algoritmo. \n",
    "\n",
    "- La función debe recibir como argumentos $\\mathbf{x}_0$, la función $f$ y \n",
    "  su gradiente, el número máximo de iteraciones $N$, la tolerancia $\\tau$, y los\n",
    "  parámetros para el algoritmo de backtracking: factor $\\rho$, la constante $c_1$\n",
    "  para la condición de descenso suficiente, la constante $c_2$ para la condición\n",
    "  de curvatura, y el máximo número de iteraciones $N_b$.\n",
    "- Agregue al algoritmo un contador\n",
    "  $nr$ que se incremente cada vez que se aplique el reinicio, es decir, cuando\n",
    "  se hace $\\beta_{k+1}=0$.\n",
    "   \n",
    "- Para calcular el tamaño de paso $\\alpha_k$ use el algoritmo de backtracking\n",
    "  usando las condiciones de Wolfe con el valor inicial $\\alpha_{ini}=1$.\n",
    "\n",
    "- Haga que la función devuelva el último punto  $\\mathbf{x}_k$, \n",
    "  el último gradiente $\\mathbf{g}_k$, el número de iteraciones $k$ \n",
    "  y una variable binaria $bres$ que indique si se cumplió el criterio\n",
    "  de paro ($bres=True$) o si el algoritmo terminó por\n",
    "  iteraciones ($bres=False$), y el contador $br·."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NONLINEAR_CONJUGATE_GRADIENT_FR(xk: np.array, f, gradf, maxiter: int,\n",
    "                                tol: float, a_init: float, p: float,\n",
    "                                c1: float, c2: float, Nb: int):\n",
    "    \"\"\"\n",
    "    THIS FINDS THE MINIMIZER OF f USING THE NON-LINEAR CONJUGATE GRADIENT METHOD WITH THE FLETCHER-REEVES FORMULA.\n",
    "\n",
    "    Args:\n",
    "    - xk:      initial guess.\n",
    "    - f:       function to minimize.\n",
    "    - gradf:   gradient of the function to minimize.\n",
    "    - maxiter: maximum number of iterations.\n",
    "    - tol:     method tolerance.\n",
    "    - a_init:  initial value for the step size in backtracking.\n",
    "    - p:       reduction factor for the step size in backtracking.\n",
    "    - c1:      parameter for the sufficient descent condition.\n",
    "    - c2:      parameter for the curvature condition.\n",
    "    - Nb:      maximum number of iterations in backtracking.\n",
    "    \n",
    "    Outputs:\n",
    "    - xk:  approach to the minimizer of f.\n",
    "    - gk:  gradient of f at xk.\n",
    "    - k:   number of iterations.\n",
    "    - T/F: if the method converged.\n",
    "    - nr:  number of restarts (bk = 0).\n",
    "    \"\"\"\n",
    "    gk = gradf(xk)\n",
    "    dk = -gk\n",
    "    nr = 0\n",
    "    for k in range(maxiter + 1):\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            return xk, gk, k, True, nr\n",
    "        ak, k1 = BACKTRAKING_WOLF(a_init = a_init, p = p, c1 = c1, c2 = c2,\n",
    "                                xk = xk, f = f, gradf = gradf, dk = dk, Nb = Nb)\n",
    "        xk = xk + ak*dk\n",
    "        gk_n = gradf(xk)\n",
    "        if np.abs(gk_n.T @ gk) < 0.2*np.linalg.norm(gk_n)**2:\n",
    "            bk = (gk_n.T @ gk_n) / (gk.T @ gk)\n",
    "        else:\n",
    "            bk = 0\n",
    "            nr += 1\n",
    "        dk = -gk_n + bk*dk\n",
    "        gk = gk_n\n",
    "    return xk, gk, maxiter, False, nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.**\n",
    "\n",
    "Pruebe el algoritmo usando la siguientes funciones con los puntos iniciales dados. Fije $N=5000$, $\\tau = \\sqrt{n}\\epsilon_m^{1/3}$, donde $n$ es la dimensión de la variable $\\mathbf{x}$ y $\\epsilon_m$ es el épsilon máquina. Para backtracking use $\\rho=0.5$, $c_1=0.001$, $c_2=0.01$, $N_b=500$. Para cada función de prueba imprima:\n",
    "   \n",
    "- la dimensión $n$,\n",
    "- $f(\\mathbf{x}_0)$,\n",
    "- el  número $k$ de iteraciones realizadas,\n",
    "- $f(\\mathbf{x}_k)$,\n",
    "- las primeras y últimas 4 entradas del punto $\\mathbf{x}_k$ que devuelve el algoritmo,\n",
    "- la norma del vector gradiente $\\mathbf{g}_k$, \n",
    "- la variable $bres$ para saber si el algoritmo puedo converger.\n",
    "- el número de reinicios $nr$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "eps_m = np.finfo(float).eps\n",
    "p = 0.5\n",
    "c1 = 0.001\n",
    "c2 = 0.01\n",
    "Nb = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de cuadrática 1:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "- $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top\\mathbf{A}_1\\mathbf{x} - \\mathbf{b}_1^\\top\\mathbf{x}$,\n",
    "  donde $\\mathbf{A}_1$ y $\\mathbf{b}_1$ están definidas como en el Ejercicio 1.\n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{10}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{100}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{1000}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    10\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  9\n",
      "f(xk):        -0.24999999999636202\n",
      "xk:           [0.05000019 0.05000019 0.05000019 0.05000019] ... [0.05000019 0.05000019 0.05000019 0.05000019]\n",
      "NORMA gk:     1.206313194339134e-05\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    9\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_FR(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    100\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  21\n",
      "f(xk):        -0.24999999999200012\n",
      "xk:           [0.00500003 0.00500003 0.00500003 0.00500003] ... [0.00500003 0.00500003 0.00500003 0.00500003]\n",
      "NORMA gk:     5.656829153792843e-05\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    21\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_FR(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    1000\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  251\n",
      "f(xk):        -0.24999999999146394\n",
      "xk:           [0.0005 0.0005 0.0005 0.0005] ... [0.0005 0.0005 0.0005 0.0005]\n",
      "NORMA gk:     0.00018476304776620826\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    251\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_FR(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de cuadrática 2:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "- $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top\\mathbf{A}_2\\mathbf{x} - \\mathbf{b}_2^\\top\\mathbf{x}$,\n",
    "  donde $\\mathbf{A}_2$ y $\\mathbf{b}_2$ están definidas como en el Ejercicio 1.\n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{10}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{100}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{1000}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    10\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  1571\n",
      "f(xk):        -1.7934207913526614\n",
      "xk:           [ 1.36889566 -1.16586292  1.60838905 -0.61279115] ... [-0.61279115  1.60838905 -1.16586292  1.36889566]\n",
      "NORMA gk:     1.8950483039484323e-05\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    1230\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_FR(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    100\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  5000\n",
      "f(xk):        -14.49428813666577\n",
      "xk:           [ 1.44208101 -1.40323557  2.08547581 -1.38702193] ... [-1.38702193  2.08547581 -1.40323557  1.44208101]\n",
      "NORMA gk:     0.0006281049138677026\n",
      "CONVERGENCIA: False\n",
      "REINICIOS:    4014\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_FR(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    1000\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  5000\n",
      "f(xk):        -141.43694656399958\n",
      "xk:           [ 1.44220913 -1.40362866  2.08622634 -1.38814564] ... [-1.38814564  2.08622634 -1.40362866  1.44220913]\n",
      "NORMA gk:     0.00029289489704420945\n",
      "CONVERGENCIA: False\n",
      "REINICIOS:    4029\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_FR(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de Beale :** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (1.5-x_1 + x_1x_2)^2 + (2.25 - x_1 + x_1x_2^2)^2 + (2.625 - x_1 + x_1x_2^3)^2.$$\n",
    "- $\\mathbf{x}_0 = (2,3)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    2\n",
      "f(x0):        3347.203125\n",
      "ITERACIONES:  78\n",
      "f(xk):        3.377431057983643e-11\n",
      "xk:           [2.99998551 0.49999649]\n",
      "NORMA gk:     6.924867590358913e-06\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    65\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2,3], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_FR(xk = x0, f = f_Beale,\n",
    "                                gradf = grad_Beale, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Beale(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Beale(xk))\n",
    "print(\"xk:          \", xk)\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de Himmelblau:** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2. $$\n",
    "- $\\mathbf{x}_0 = (2,4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    2\n",
      "f(x0):        130.0\n",
      "ITERACIONES:  37\n",
      "f(xk):        2.0568411381419677e-13\n",
      "xk:           [ 3.58442828 -1.84812653]\n",
      "NORMA gk:     6.585280676690073e-06\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    36\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2,4], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_FR(xk = x0, f = f_Himmelblau,\n",
    "                                gradf = grad_Himmelblau, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Himmelblau(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Himmelblau(xk))\n",
    "print(\"xk:          \", xk)\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de Rosenbrock:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (1-x_i)^2 \\right]\n",
    "\\quad n\\geq 2.$$\n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0)\\in \\mathbb{R}^{2}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{20}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{40}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    2\n",
      "f(x0):        24.199999999999996\n",
      "ITERACIONES:  5000\n",
      "f(xk):        7.904920227577363e-10\n",
      "xk:           [1.00002811 1.0000562 ]\n",
      "NORMA gk:     6.73075488715537e-05\n",
      "CONVERGENCIA: False\n",
      "REINICIOS:    4249\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2, 1.0], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_FR(xk = x0, f = f_Rosenbrock,\n",
    "                                gradf = grad_Rosenbrock, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Rosenbrock(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Rosenbrock(xk))\n",
    "print(\"xk:          \", xk)\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    20\n",
      "f(x0):        4598.000000000001\n",
      "ITERACIONES:  956\n",
      "f(xk):        2.0658248626373865e-11\n",
      "xk:           [1.         0.99999999 1.         0.99999999] ... [0.99999903 0.99999805 0.99999609 0.99999215]\n",
      "NORMA gk:     2.5601483353585578e-05\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    788\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_FR(xk = x0, f = f_Rosenbrock,\n",
    "                                gradf = grad_Rosenbrock, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Rosenbrock(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Rosenbrock(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    40\n",
      "f(x0):        9680.000000000002\n",
      "ITERACIONES:  2681\n",
      "f(xk):        1.6050071699166508e-10\n",
      "xk:           [1. 1. 1. 1.] ... [0.99999727 0.99999454 0.99998906 0.99997807]\n",
      "NORMA gk:     3.221647914928825e-05\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    2292\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_FR(xk = x0, f = f_Rosenbrock,\n",
    "                                gradf = grad_Rosenbrock, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Rosenbrock(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Rosenbrock(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **3.- Ejercicio 3  :**\n",
    "\n",
    "---\n",
    "\n",
    "Programar el método de gradiente conjugado no lineal de usando la fórmula de\n",
    "Hestenes-Stiefel:\n",
    "\n",
    "En este caso el algoritmo es igual al del Ejercicio 2, con excepción del cálculo de $\\beta_{k+1}$. Primero se calcula el vector $\\mathbf{y}_k$ y luego $\\beta_{k+1}$:\n",
    "\n",
    "$$ \\mathbf{y}_k =  \\nabla f_{k+1}-\\nabla f_{k} $$\n",
    "$$ \\beta_{k+1} =   \\frac{\\nabla f_{k+1}^\\top\\mathbf{y}_k }{\\nabla p_{k}^\\top\\mathbf{y}_k}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NONLINEAR_CONJUGATE_GRADIENT_HS(xk: np.array, f, gradf, maxiter: int,\n",
    "                                tol: float, a_init: float, p: float,\n",
    "                                c1: float, c2: float, Nb: int):\n",
    "    \"\"\"\n",
    "    THIS FINDS THE MINIMIZER OF f USING THE NON-LINEAR CONJUGATE GRADIENT METHOD WITH THE HESTENES-STIEFEL FORMULA.\n",
    "\n",
    "    Args:\n",
    "    - xk:      initial guess.\n",
    "    - f:       function to minimize.\n",
    "    - gradf:   gradient of the function to minimize.\n",
    "    - maxiter: maximum number of iterations.\n",
    "    - tol:     method tolerance.\n",
    "    - a_init:  initial value for the step size in backtracking.\n",
    "    - p:       reduction factor for the step size in backtracking.\n",
    "    - c1:      parameter for the sufficient descent condition.\n",
    "    - c2:      parameter for the curvature condition.\n",
    "    - Nb:      maximum number of iterations in backtracking.\n",
    "    \n",
    "    Outputs:\n",
    "    - xk:  approach to the minimizer of f.\n",
    "    - gk:  gradient of f at xk.\n",
    "    - k:   number of iterations.\n",
    "    - T/F: if the method converged.\n",
    "    - nr:  number of restarts (bk = 0).\n",
    "    \"\"\"\n",
    "    gk = gradf(xk)\n",
    "    dk = -gk\n",
    "    nr = 0\n",
    "    for k in range(maxiter + 1):\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            return xk, gk, k, True, nr\n",
    "        ak, k1 = BACKTRAKING_WOLF(a_init = a_init, p = p, c1 = c1, c2 = c2,\n",
    "                                xk = xk, f = f, gradf = gradf, dk = dk, Nb = Nb)\n",
    "        xk = xk + ak*dk\n",
    "        gk_n = gradf(xk)\n",
    "        yk = gk_n - gk\n",
    "        if np.abs(gk_n.T @ gk) < 0.2*np.linalg.norm(gk_n)**2:\n",
    "            bk = (gk_n.T @ yk) / (dk.T @ yk)\n",
    "        else:\n",
    "            bk = 0\n",
    "            nr += 1\n",
    "        dk = -gk_n + bk*dk\n",
    "        gk = gk_n\n",
    "    return xk, gk, maxiter, False, nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1.**\n",
    "Repita el Ejercicio 2 usando la fórmula de Hestenes-Stiefel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "eps_m = np.finfo(float).eps\n",
    "p = 0.5\n",
    "c1 = 0.001\n",
    "c2 = 0.01\n",
    "Nb = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de cuadrática 1:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "- $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top\\mathbf{A}_1\\mathbf{x} - \\mathbf{b}_1^\\top\\mathbf{x}$,\n",
    "  donde $\\mathbf{A}_1$ y $\\mathbf{b}_1$ están definidas como en el Ejercicio 1.\n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{10}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{100}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{1000}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    10\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  9\n",
      "f(xk):        -0.24999999999636202\n",
      "xk:           [0.05000019 0.05000019 0.05000019 0.05000019] ... [0.05000019 0.05000019 0.05000019 0.05000019]\n",
      "NORMA gk:     1.206313194339134e-05\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    9\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_HS(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    100\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  21\n",
      "f(xk):        -0.24999999999200012\n",
      "xk:           [0.00500003 0.00500003 0.00500003 0.00500003] ... [0.00500003 0.00500003 0.00500003 0.00500003]\n",
      "NORMA gk:     5.656829153792843e-05\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    21\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_HS(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    1000\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  251\n",
      "f(xk):        -0.24999999999146394\n",
      "xk:           [0.0005 0.0005 0.0005 0.0005] ... [0.0005 0.0005 0.0005 0.0005]\n",
      "NORMA gk:     0.00018476304776620826\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    251\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_HS(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de cuadrática 2:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "- $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top\\mathbf{A}_2\\mathbf{x} - \\mathbf{b}_2^\\top\\mathbf{x}$,\n",
    "  donde $\\mathbf{A}_2$ y $\\mathbf{b}_2$ están definidas como en el Ejercicio 1.\n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{10}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{100}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{1000}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    10\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  1571\n",
      "f(xk):        -1.7934207913526614\n",
      "xk:           [ 1.36889566 -1.16586292  1.60838905 -0.61279115] ... [-0.61279115  1.60838905 -1.16586292  1.36889566]\n",
      "NORMA gk:     1.8950483039484323e-05\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    1230\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_HS(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    100\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  5000\n",
      "f(xk):        -14.494146346147428\n",
      "xk:           [ 1.43574711 -1.38364255  2.04861357 -1.33199051] ... [-1.33199051  2.04861357 -1.38364255  1.43574711]\n",
      "NORMA gk:     0.0012286586322716373\n",
      "CONVERGENCIA: False\n",
      "REINICIOS:    4035\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_HS(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_HS(xk = x0, f = f_cuad,\n",
    "                                gradf = gradf_cuad, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de Beale :** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (1.5-x_1 + x_1x_2)^2 + (2.25 - x_1 + x_1x_2^2)^2 + (2.625 - x_1 + x_1x_2^3)^2.$$\n",
    "- $\\mathbf{x}_0 = (2,3)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    2\n",
      "f(x0):        3347.203125\n",
      "ITERACIONES:  769\n",
      "f(xk):        5.6113870648033834e-11\n",
      "xk:           [3.00001871 0.50000457]\n",
      "NORMA gk:     7.496323664379619e-06\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    585\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2,3], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_HS(xk = x0, f = f_Beale,\n",
    "                                gradf = grad_Beale, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Beale(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Beale(xk))\n",
    "print(\"xk:          \", xk)\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de Himmelblau:** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2. $$\n",
    "- $\\mathbf{x}_0 = (2,4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    2\n",
      "f(x0):        130.0\n",
      "ITERACIONES:  37\n",
      "f(xk):        1.9833788488942771e-13\n",
      "xk:           [ 3.58442828 -1.84812653]\n",
      "NORMA gk:     6.466611484709132e-06\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    36\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2,4], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_HS(xk = x0, f = f_Himmelblau,\n",
    "                                gradf = grad_Himmelblau, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Himmelblau(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Himmelblau(xk))\n",
    "print(\"xk:          \", xk)\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de Rosenbrock:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (1-x_i)^2 \\right]\n",
    "\\quad n\\geq 2.$$\n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0)\\in \\mathbb{R}^{2}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{20}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{40}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    2\n",
      "f(x0):        24.199999999999996\n",
      "ITERACIONES:  1382\n",
      "f(xk):        1.606990862536676e-11\n",
      "xk:           [0.999996   0.99999198]\n",
      "NORMA gk:     8.384887632319202e-06\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    1128\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2, 1.0], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_HS(xk = x0, f = f_Rosenbrock,\n",
    "                                gradf = grad_Rosenbrock, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Rosenbrock(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Rosenbrock(xk))\n",
    "print(\"xk:          \", xk)\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    20\n",
      "f(x0):        4598.000000000001\n",
      "ITERACIONES:  1618\n",
      "f(xk):        2.104455538164449e-10\n",
      "xk:           [1. 1. 1. 1.] ... [0.99999688 0.99999374 0.99998745 0.99997483]\n",
      "NORMA gk:     2.615760044191093e-05\n",
      "CONVERGENCIA: True\n",
      "REINICIOS:    1250\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_HS(xk = x0, f = f_Rosenbrock,\n",
    "                                gradf = grad_Rosenbrock, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Rosenbrock(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Rosenbrock(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    40\n",
      "f(x0):        9680.000000000002\n",
      "ITERACIONES:  5000\n",
      "f(xk):        0.11268245259510778\n",
      "xk:           [1.00000798 0.99998119 1.00003116 0.99995921] ... [0.92147857 0.84769665 0.71728795 0.51265151]\n",
      "NORMA gk:     1.2311603097582053\n",
      "CONVERGENCIA: False\n",
      "REINICIOS:    3977\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps_m**(1/3)\n",
    "xk, gk, k, conv, nr = NONLINEAR_CONJUGATE_GRADIENT_HS(xk = x0, f = f_Rosenbrock,\n",
    "                                gradf = grad_Rosenbrock, maxiter = N, tol = tol,\n",
    "                                a_init = 1, p = p, c1 = c1, c2 = c2, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Rosenbrock(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Rosenbrock(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"NORMA gk:    \", np.linalg.norm(gk))\n",
    "print(\"CONVERGENCIA:\", conv)\n",
    "print(\"REINICIOS:   \", nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2.**\n",
    "\n",
    "¿Hay alguna diferencia que indique que es mejor usar la fórmula de Hestenes-Stiefel\n",
    "   respesto a Fletcher-Reeves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Comparando el rendimiento del algoritmo con ambas fórmulas usando exactamente los mismos parámetros, se puede notar que se tiene el mismo rendimiento en la función cuadrática 1, llegando a la misma solución por ambos métodos y coincidiendo en la convergencia. En la función cuadrática 2 se tuvo un desempeño similar, con algunas diferencias en el número de reinicios, ligeramente en favor de la fórmula de Fletcher-Reeves.\n",
    ">\n",
    ">En las función de Beale, la fórmula de Fletcher-Reeves tuvo mejor desempeño mientras que en Rosenbrock, Hestenes-Stiefel logró la convergencia del algoritmo en los casos donde Fletcher-Reeves no.\n",
    ">\n",
    ">En términos generales, Fletcher-Reeves tiene mejor desempeño ya que en prácticamente todos los casos tuvo una cantidad de iteraciones menor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3.**\n",
    "\n",
    "La cantidad de reinicios puede indicar que tanto se comporta el algoritmo\n",
    "   como el algoritmo de descenso máximo. Agregue un comentario sobre esto \n",
    "   de acuerdo a los resultados obtenidos para cada fórmula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Comparando la cantidad de reincios de cada implementación, se obutvieron los siguientes resultados:\n",
    "\n",
    "| Fletcher-Reeves | Hestenes-Stiefel |\n",
    "|--------|--------|\n",
    "| 9      | 9      |\n",
    "| 21     | 21     |\n",
    "| 251    | 251    |\n",
    "| 1230   | 1230   |\n",
    "| 4014   | 4035   |\n",
    "| 4029   | 4022   |\n",
    "| 65     | 585    |\n",
    "| 36     | 36     |\n",
    "| 4249   | 1128   |\n",
    "| 788    | 1250   |\n",
    "| 2292   | 3977   |\n",
    "\n",
    "\n",
    "> Aquí podemos notar que, en términos generales, la fórmula de Fletcher-Reeves usa menor cantidad de reinicios, es decir se comporta más como el algoritmo de descenso máximo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

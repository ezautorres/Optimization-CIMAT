{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <img src=\"https://th.bing.com/th/id/R.3cd1c8dc996c5616cf6e65e20b6bf586?rik=09aaLyk4hfbBiQ&riu=http%3a%2f%2fcidics.uanl.mx%2fwp-content%2fuploads%2f2016%2f09%2fcimat.png&ehk=%2b0brgMUkA2BND22ixwLZheQrrOoYLO3o5cMRqsBOrlY%3d&risl=&pid=ImgRaw&r=0\" \n",
    "     style=\"float: right; margin-right: 30px;\" \n",
    "     width=\"120\"\n",
    "     />\n",
    "\n",
    " ---\n",
    " \n",
    " # **OPTIMIZACIÓN: TAREA 7**\n",
    " EZAU FARIDH TORRES TORRES.\n",
    "     \n",
    "<p align=\"right\"> Maestría en Ciencias con Orientación en Matemáticas Aplicadas. </p>\n",
    "<p align=\"right\"> CENTRO DE INVESTIGACIÓN EN MATEMÁTICAS. </p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = \"dark\")\n",
    "\n",
    "def BACKTRAKING(alpha_i: float, p: float, c: float, \n",
    "                xk: np.array, f, fxk: np.array,\n",
    "                gradfxk: np.array, pk: np.array, Nb: int):\n",
    "    alpha = alpha_i\n",
    "    for i in range(Nb):\n",
    "        if f(xk + alpha*pk) <= fxk + c*alpha*(gradfxk.T)@pk:\n",
    "            return alpha, i\n",
    "        alpha = p*alpha\n",
    "    return alpha, Nb\n",
    "\n",
    "def f_Himmelblau(x: np.array):\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "def grad_Himmelblau(x: np.array):\n",
    "    x1 = 4*x[0]*(x[0]**2 + x[1] - 11) + 2*(x[0] + x[1]**2 - 7)\n",
    "    x2 = 2*(x[0]**2 + x[1] - 11) + 4*x[1]*(x[0] + x[1]**2 - 7)\n",
    "    return np.array([x1,x2], dtype = float)\n",
    "def Hess_Himmelblau(x: np.array):\n",
    "    x11 = 12*x[0]**2 + 4*x[1] - 42\n",
    "    x12 = 4*x[0] + 4*x[1]\n",
    "    x22 = 4*x[0] + 12*x[1]**2 - 26\n",
    "    return np.array([[x11, x12], [x12, x22]], dtype = float)\n",
    "\n",
    "def f_Beale(x: np.array):\n",
    "    return (1.5 - x[0] + x[0]*x[1])**2 + (2.25 - x[0] + x[0]*x[1]**2)**2 + (2.625 - x[0] + x[0]*x[1]**3)**2\n",
    "def grad_Beale(x: np.array):\n",
    "    x1 = 2*(x[1] - 1)*(1.5 - x[0] + x[0]*x[1]) + 2*(x[1]**2 - 1)*(2.25 - x[0] + x[0]*x[1]**2) + 2*(x[1]**3 - 1)*(2.625 - x[0] + x[0]*x[1]**3)\n",
    "    x2 = 2*x[0]*(1.5 - x[0] + x[0]*x[1]) + 4*x[0]*x[1]*(2.25 - x[0] + x[0]*x[1]**2) + 6*x[0]*(x[1]**2)*(2.625 - x[0] + x[0]*x[1]**3)\n",
    "    return np.array([x1,x2], dtype = float)\n",
    "def Hess_Beale(x: np.array):\n",
    "    x11 = 2*(x[1]**3 - 1)**2 + 2*(x[1]**2 - 1)**2 + 2*(x[1] - 1)**2\n",
    "    x12 = 4*x[0]*x[1]*(x[1]**2 - 1) + 4*x[1]*(x[0]*x[1]**2 - x[0]+2.25) + 6*x[0]*x[1]**2*(x[1]**3 - 1) + 6*x[1]**2*(x[0]*x[1]**3 - x[0]+2.625) + 2*x[0]*(x[1]-1) + 2*(x[0]*x[1] - x[0]+1.5)\n",
    "    x22 = 18*x[0]**2*x[1]**4 + 8*x[0]**2*x[1]**2 + 2*x[0]**2 + 12*x[0]*x[1]*(x[0]*x[1]**3 - x[0] + 2.625) + 4*x[0]*(x[0]*x[1]**2 - x[0]+2.25)\n",
    "    return np.array([[x11, x12], [x12, x22]], dtype = float)\n",
    "\n",
    "def f_Rosenbrock(x: np.array):\n",
    "    n = len(x)\n",
    "    s = 0\n",
    "    for i in range(n-1):\n",
    "        s = s + 100*(x[i+1] - x[i]**2)**2 + (1 - x[i])**2\n",
    "    return s\n",
    "def grad_Rosenbrock(x: np.array):\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    grad[0] = -400*x[0]*(x[1] - x[0]**2) - 2*(1-x[0])\n",
    "    grad[n-1] = 200*(x[n-1] - x[n-2]**2)\n",
    "    for j in range(1,n-1):\n",
    "        grad[j] = 200*(x[j]-x[j-1]**2) - 400*x[j]*(x[j+1] - x[j]**2) - 2*(1-x[j])\n",
    "    return np.array(grad, dtype = float)\n",
    "def Hess_Rosenbrock(x: np.array):\n",
    "    n = len(x)\n",
    "    Hess = np.zeros((n,n))\n",
    "    Hess[0,0] = -400*(x[1]-x[0]**2) + 800*x[0]**2 + 2\n",
    "    Hess[1,0] = -400*x[0]\n",
    "    Hess[n-2,n-1] = -400*x[n-2]\n",
    "    Hess[n-1,n-1] = 200\n",
    "    for j in range(1,n-1):\n",
    "        Hess[j-1,j] = -400*x[j-1]\n",
    "        Hess[j,j] = -400*(x[j+1]-x[j]**2) +800*x[j]**2 + 202\n",
    "        Hess[j+1,j] = -400*x[j]\n",
    "    return np.array(Hess, dtype = float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **1.- EJERCICIO 1:**\n",
    "\n",
    "---\n",
    "Programar el método de Newton truncado descrito en el Algoritmo 1 y 2 de la Clase 20.\n",
    "   \n",
    "## **1.1.**\n",
    "Programar la función que implementa el Algoritmo 1, que calcula una aproximación\n",
    "   de la solución del sistema de Newton. \n",
    "- Haga que la función devuelva la dirección $\\mathbf{p}_k$ y el número de iteraciones realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CG(gk: np.array, Hk: np.array, n: int, tol: float):\n",
    "    \"\"\"\n",
    "    CONJUGATE GRADIENT APPLIED TO NEWTON SYSTEM.\n",
    "\n",
    "    Args:\n",
    "    - gk:  gradient of f at xk.\n",
    "    - Hk:  Hessian of f at xk.\n",
    "    - n:   dimention.\n",
    "    - tol: tolerance.\n",
    "\n",
    "    Outputs:\n",
    "    - pk: search direction.\n",
    "    - j:  number of iterations.\n",
    "    \"\"\"\n",
    "    zj = 0\n",
    "    rj = gk\n",
    "    dj = -rj\n",
    "    for j in range(n-1):\n",
    "        if dj.T @ Hk @ dj <= 0:\n",
    "            if j == 0:\n",
    "                pk = -gk\n",
    "            else:\n",
    "                pk = zj\n",
    "        aj = (rj.T @ rj) / (dj.T @ Hk @ dj)\n",
    "        zj = zj + aj * dj\n",
    "        rj_n = rj + aj * Hk @ dj\n",
    "        Bj = (rj_n.T @ rj_n) / (rj.T @ rj)\n",
    "        dj = -rj_n + Bj * dj\n",
    "        if np.linalg.norm(rj_n) < tol:\n",
    "            pk = zj\n",
    "            return pk, j\n",
    "        rj = rj_n\n",
    "    pk = zj\n",
    "    return pk, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2.**\n",
    "Programar la función que implementa el Algoritmo 2. \n",
    "- Use el algoritmo de backtracking con la condición de descenso suficiente para \n",
    "  calcular el tamaño de paso $\\alpha_k$.\n",
    "- Defina la variable binaria $res$ de modo que $True$ si se cumple la condición de salida\n",
    "  $\\|\\mathbf{g}_k\\|<\\tau$ y $False$ si termina por iteraciones.\n",
    "- Calcule el promedio de las iteraciones realizadas por el Algoritmo 1 \n",
    "- Haga que la función devuelva $\\mathbf{x}_k, \\mathbf{g}_k, k, res$ y el\n",
    "  promedio de la iteraciones realizadas por el Algoritmo 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NEWTON_CG_LINESEARCH_METHOD(f, gradf, Hessf, xk: np.array, tol: float, maxiter: int,\n",
    "                                alpha_i: float, p: float, c: float, Nb: int):\n",
    "    \"\"\"\n",
    "    NEWTON CONJUGATE GRADIENT METHOD WITH LINE SEARCH.\n",
    "\n",
    "    Args:\n",
    "    - f:       function to minimize.\n",
    "    - gradf:   gradient of f.\n",
    "    - Hessf:   Hessian of f.\n",
    "    - xk:      initial point.\n",
    "    - tol:     tolerance.\n",
    "    - maxiter: maximum number of iterations.\n",
    "    - alpha_i: initial alpha.\n",
    "    - p:       reduction factor.\n",
    "    - c:       constant for Armijo condition.\n",
    "    - Nb:      maximum number of iterations for line search.\n",
    "\n",
    "    Outputs:\n",
    "    - xk:   optimal point.\n",
    "    - gk:   gradient at xk.\n",
    "    - k:    number of iterations.\n",
    "    - T/F:  if the method converged.\n",
    "    - MEAN: average number of iterations for line search.\n",
    "    \"\"\"\n",
    "    n = len(xk)\n",
    "    iteraciones = []\n",
    "    for k in range(maxiter):\n",
    "        gk = gradf(xk)\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            return xk, gk, k, True, np.mean(iteraciones)\n",
    "        Hk = Hessf(xk)\n",
    "        ek = min(0.5, np.sqrt(np.linalg.norm(gk)))*np.sqrt(np.linalg.norm(gk))\n",
    "        pk, i = CG(gk, Hk, n, ek)\n",
    "        ak = BACKTRAKING(alpha_i = alpha_i, p = p, c = c, xk = xk, f = f,\n",
    "                            fxk=f(xk), gradfxk = gk, pk = pk, Nb = Nb)[0]\n",
    "        iteraciones.append(i)\n",
    "        xk = xk + ak * pk\n",
    "    return xk, gk, maxiter, False, np.mean(iteraciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3.**\n",
    "Pruebe el algoritmo para minimizar las siguientes funciones usando los parámetros\n",
    "   $N=5000$, $\\tau = \\sqrt{n}\\epsilon_m^{1/3}$, donde $n$ es la dimensión\n",
    "   de la variable $\\mathbf{x}$ y $\\epsilon_m$ es el épsilon máquina. \n",
    "   Para backtracking use $\\rho=0.5$, $c_1=0.001$ y el número máximo de iteraciones $N_b=500$.\n",
    "   \n",
    "   En cada caso imprima los siguientes datos:\n",
    "   \n",
    "- la dimensión $n$,\n",
    "- $f(\\mathbf{x}_0)$,\n",
    "- el  número $k$ de iteraciones realizadas,\n",
    "- $f(\\mathbf{x}_k)$,\n",
    "- las primeras y últimas 4 entradas del punto $\\mathbf{x}_k$ que devuelve el algoritmo,\n",
    "- la norma del vector gradiente $\\mathbf{g}_k$, \n",
    "- el promedio del número de iteraciones realizadas por el Algoritmo 1.\n",
    "- la variable $res$ para saber si el algoritmo puedo converger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "eps = np.finfo(float).eps\n",
    "p = 0.5\n",
    "c = 0.001\n",
    "Nb = 500\n",
    "alpha_i = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Función cuadrática 1:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "- $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top\\mathbf{A}_1\\mathbf{x} - \\mathbf{b}_1^\\top\\mathbf{x}$,\n",
    "  donde $\\mathbf{A}_1$ y $\\mathbf{b}_1$ están definidas por\n",
    "  \n",
    "\n",
    "$$ \\mathbf{A}_1 = n\\mathbf{I} + \\mathbf{1} = \n",
    "\\left[\\begin{array}{llll} n      & 0      & \\cdots & 0 \\\\\n",
    "                       0      & n      & \\cdots & 0 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       0      & 0      & \\cdots & n \\end{array}\\right]\n",
    "+ \\left[\\begin{array}{llll} 1    & 1      & \\cdots & 1 \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\end{array}\\right],  \\qquad\n",
    "\\mathbf{b}_1 = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right], $$\n",
    "\n",
    "donde $\\mathbf{I}$ es la matriz identidad y $\\mathbf{1}$ es la matriz llena de 1's,\n",
    "ambas de tamaño $n$, usando los puntos iniciales   \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{10}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{100}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{1000}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    10\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  1\n",
      "f(xk):        -0.24999999999999994\n",
      "xk:           [0.05 0.05 0.05 0.05] ... [0.05 0.05 0.05 0.05]\n",
      "||gk||:       6.280369834735101e-16\n",
      "PROMEDIO:     0.0\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "Hessf_cuad = lambda x: A1\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps**(1/3)\n",
    "xk, gk, k, RES, MEAN = NEWTON_CG_LINESEARCH_METHOD(f = f_cuad, gradf = gradf_cuad,\n",
    "                                                   Hessf = Hessf_cuad, xk = x0, tol = tol,\n",
    "                                                   maxiter = N, alpha_i = alpha_i, p = p,\n",
    "                                                   c = c, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"||gk||:      \", np.linalg.norm(gk))\n",
    "print(\"PROMEDIO:    \", MEAN)\n",
    "print(\"CONVERGENCIA:\", RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    100\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  1\n",
      "f(xk):        -0.25000000000000006\n",
      "xk:           [0.005 0.005 0.005 0.005] ... [0.005 0.005 0.005 0.005]\n",
      "||gk||:       7.691850745534255e-16\n",
      "PROMEDIO:     0.0\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "Hessf_cuad = lambda x: A1\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps**(1/3)\n",
    "xk, gk, k, RES, MEAN = NEWTON_CG_LINESEARCH_METHOD(f = f_cuad, gradf = gradf_cuad,\n",
    "                                                   Hessf = Hessf_cuad, xk = x0, tol = tol,\n",
    "                                                   maxiter = N, alpha_i = alpha_i, p = p,\n",
    "                                                   c = c, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"||gk||:      \", np.linalg.norm(gk))\n",
    "print(\"PROMEDIO:    \", MEAN)\n",
    "print(\"CONVERGENCIA:\", RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    1000\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  1\n",
      "f(xk):        -0.2500000000000049\n",
      "xk:           [0.0005 0.0005 0.0005 0.0005] ... [0.0005 0.0005 0.0005 0.0005]\n",
      "||gk||:       1.192009396658756e-13\n",
      "PROMEDIO:     0.0\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "Hessf_cuad = lambda x: A1\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps**(1/3)\n",
    "xk, gk, k, RES, MEAN = NEWTON_CG_LINESEARCH_METHOD(f = f_cuad, gradf = gradf_cuad,\n",
    "                                                   Hessf = Hessf_cuad, xk = x0, tol = tol,\n",
    "                                                   maxiter = N, alpha_i = alpha_i, p = p,\n",
    "                                                   c = c, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"||gk||:      \", np.linalg.norm(gk))\n",
    "print(\"PROMEDIO:    \", MEAN)\n",
    "print(\"CONVERGENCIA:\", RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Función de cuadrática 2:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "- $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top\\mathbf{A}_2\\mathbf{x} - \\mathbf{b}_2^\\top\\mathbf{x}$,\n",
    "  donde $\\mathbf{A}_2= [a_{ij}]$ y $\\mathbf{b}_2$ están definidas por\n",
    "  \n",
    "$$ a_{ij} = exp\\left(-0.25(i-j)^2 \\right),  \\qquad\n",
    "\\mathbf{b}_2 = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] $$\n",
    "\n",
    "usando los puntos iniciales:\n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{10}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{100}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{1000}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    10\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  22\n",
      "f(xk):        -1.7934208015015511\n",
      "xk:           [ 1.36906831 -1.16629192  1.6089342  -0.61322014] ... [-0.61322014  1.6089342  -1.16629192  1.36906831]\n",
      "||gk||:       1.5307316549914036e-05\n",
      "PROMEDIO:     0.3181818181818182\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "Hessf_cuad = lambda x: A2\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps**(1/3)\n",
    "xk, gk, k, RES, MEAN = NEWTON_CG_LINESEARCH_METHOD(f = f_cuad, gradf = gradf_cuad,\n",
    "                                                   Hessf = Hessf_cuad, xk = x0, tol = tol,\n",
    "                                                   maxiter = N, alpha_i = alpha_i, p = p,\n",
    "                                                   c = c, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"||gk||:      \", np.linalg.norm(gk))\n",
    "print(\"PROMEDIO:    \", MEAN)\n",
    "print(\"CONVERGENCIA:\", RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    100\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  97\n",
      "f(xk):        -14.494331024801777\n",
      "xk:           [ 1.44646083 -1.41684769  2.11130648 -1.42597653] ... [-1.42597732  2.11130208 -1.41684881  1.44646501]\n",
      "||gk||:       6.0175662526390146e-05\n",
      "PROMEDIO:     2.3402061855670104\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "Hessf_cuad = lambda x: A2\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps**(1/3)\n",
    "xk, gk, k, RES, MEAN = NEWTON_CG_LINESEARCH_METHOD(f = f_cuad, gradf = gradf_cuad,\n",
    "                                                   Hessf = Hessf_cuad, xk = x0, tol = tol,\n",
    "                                                   maxiter = N, alpha_i = alpha_i, p = p,\n",
    "                                                   c = c, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"||gk||:      \", np.linalg.norm(gk))\n",
    "print(\"PROMEDIO:    \", MEAN)\n",
    "print(\"CONVERGENCIA:\", RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    1000\n",
      "f(x0):        0.0\n",
      "ITERACIONES:  246\n",
      "f(xk):        -141.43698480430896\n",
      "xk:           [ 1.4468278  -1.41800279  2.11341523 -1.42884304] ... [-1.42883692  2.11339703 -1.41801106  1.44684836]\n",
      "||gk||:       0.0001753439970808574\n",
      "PROMEDIO:     0.4715447154471545\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "Hessf_cuad = lambda x: A2\n",
    "x0 = np.zeros(n, dtype = float)\n",
    "tol = np.sqrt(n)*eps**(1/3)\n",
    "xk, gk, k, RES, MEAN = NEWTON_CG_LINESEARCH_METHOD(f = f_cuad, gradf = gradf_cuad,\n",
    "                                                   Hessf = Hessf_cuad, xk = x0, tol = tol,\n",
    "                                                   maxiter = N, alpha_i = alpha_i, p = p,\n",
    "                                                   c = c, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_cuad(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_cuad(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"||gk||:      \", np.linalg.norm(gk))\n",
    "print(\"PROMEDIO:    \", MEAN)\n",
    "print(\"CONVERGENCIA:\", RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Función de Beale :** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (1.5-x_1 + x_1x_2)^2 + (2.25 - x_1 + x_1x_2^2)^2 + (2.625 - x_1 + x_1x_2^3)^2.$$\n",
    "- $\\mathbf{x}_0 = (2,3)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    2\n",
      "f(x0):        3347.203125\n",
      "ITERACIONES:  303\n",
      "f(xk):        1.0406054905540409e-10\n",
      "xk:           [2.99997453 0.49999359]\n",
      "||gk||:       8.270842722018332e-06\n",
      "PROMEDIO:     1.0165016501650166\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2,3], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps**(1/3)\n",
    "xk, gk, k, RES, MEAN = NEWTON_CG_LINESEARCH_METHOD(f = f_Beale, gradf = grad_Beale,\n",
    "                                                   Hessf = Hess_Beale, xk = x0, tol = tol,\n",
    "                                                   maxiter = N, alpha_i = alpha_i, p = p,\n",
    "                                                   c = c, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Beale(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Beale(xk))\n",
    "print(\"xk:          \", xk)\n",
    "print(\"||gk||:      \", np.linalg.norm(gk))\n",
    "print(\"PROMEDIO:    \", MEAN)\n",
    "print(\"CONVERGENCIA:\", RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Función de Himmelblau:** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2. $$\n",
    "- $\\mathbf{x}_0 = (2,4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    2\n",
      "f(x0):        130.0\n",
      "ITERACIONES:  19\n",
      "f(xk):        5.260828354624073e-13\n",
      "xk:           [2.99999987 2.00000009]\n",
      "||gk||:       7.77271199768923e-06\n",
      "PROMEDIO:     0.42105263157894735\n",
      "CONVERGENCIA: True\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2,4], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps**(1/3)\n",
    "xk, gk, k, RES, MEAN = NEWTON_CG_LINESEARCH_METHOD(f = f_Himmelblau, gradf = grad_Himmelblau,\n",
    "                                                   Hessf = Hess_Himmelblau, xk = x0, tol = tol,\n",
    "                                                   maxiter = N, alpha_i = alpha_i, p = p,\n",
    "                                                   c = c, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Himmelblau(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Himmelblau(xk))\n",
    "print(\"xk:          \", xk)\n",
    "print(\"||gk||:      \", np.linalg.norm(gk))\n",
    "print(\"PROMEDIO:    \", MEAN)\n",
    "print(\"CONVERGENCIA:\", RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Función de Rosenbrock:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (1-x_i)^2 \\right]\n",
    "\\quad n\\geq 2.$$\n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0)\\in \\mathbb{R}^{2}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{20}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{40}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    2\n",
      "f(x0):        24.199999999999996\n",
      "ITERACIONES:  5000\n",
      "f(xk):        2.4516036579601876\n",
      "xk:           [-0.56382959  0.32567598]\n",
      "||gk||:       2.075163968829345\n",
      "PROMEDIO:     1.9996\n",
      "CONVERGENCIA: False\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2, 1.0], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps**(1/3)\n",
    "xk, gk, k, RES, MEAN = NEWTON_CG_LINESEARCH_METHOD(f = f_Rosenbrock, gradf = grad_Rosenbrock,\n",
    "                                                   Hessf = Hess_Rosenbrock, xk = x0, tol = tol,\n",
    "                                                   maxiter = N, alpha_i = alpha_i, p = p,\n",
    "                                                   c = c, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Rosenbrock(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Rosenbrock(xk))\n",
    "print(\"xk:          \", xk)\n",
    "print(\"||gk||:      \", np.linalg.norm(gk))\n",
    "print(\"PROMEDIO:    \", MEAN)\n",
    "print(\"CONVERGENCIA:\", RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    20\n",
      "f(x0):        4598.000000000001\n",
      "ITERACIONES:  5000\n",
      "f(xk):        19.4882350039715\n",
      "xk:           [-0.61878307  0.39649438  0.16675802  0.03844237] ... [1.03156251e-02 1.00512735e-02 1.01400139e-02 7.43921058e-06]\n",
      "||gk||:       0.5140967920786855\n",
      "PROMEDIO:     3.0002\n",
      "CONVERGENCIA: False\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps**(1/3)\n",
    "xk, gk, k, RES, MEAN = NEWTON_CG_LINESEARCH_METHOD(f = f_Rosenbrock, gradf = grad_Rosenbrock,\n",
    "                                                   Hessf = Hess_Rosenbrock, xk = x0, tol = tol,\n",
    "                                                   maxiter = N, alpha_i = alpha_i, p = p,\n",
    "                                                   c = c, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Rosenbrock(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Rosenbrock(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"||gk||:      \", np.linalg.norm(gk))\n",
    "print(\"PROMEDIO:    \", MEAN)\n",
    "print(\"CONVERGENCIA:\", RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSION:    40\n",
      "f(x0):        9680.000000000002\n",
      "ITERACIONES:  5000\n",
      "f(xk):        39.28658778440892\n",
      "xk:           [-0.61806366  0.39558009  0.16620719  0.03840828] ... [0.01029175 0.00991944 0.01042095 0.00019364]\n",
      "||gk||:       0.5194618790851864\n",
      "PROMEDIO:     3.0002\n",
      "CONVERGENCIA: False\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0, -1.2, 1.0], dtype = float)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*eps**(1/3)\n",
    "xk, gk, k, RES, MEAN = NEWTON_CG_LINESEARCH_METHOD(f = f_Rosenbrock, gradf = grad_Rosenbrock,\n",
    "                                                   Hessf = Hess_Rosenbrock, xk = x0, tol = tol,\n",
    "                                                   maxiter = N, alpha_i = alpha_i, p = p,\n",
    "                                                   c = c, Nb = Nb)\n",
    "print(\"DIMENSION:   \", n)\n",
    "print(\"f(x0):       \", f_Rosenbrock(x0))\n",
    "print(\"ITERACIONES: \", k)\n",
    "print(\"f(xk):       \", f_Rosenbrock(xk))\n",
    "print(\"xk:          \", xk[:4], \"...\", xk[-4:])\n",
    "print(\"||gk||:      \", np.linalg.norm(gk))\n",
    "print(\"PROMEDIO:    \", MEAN)\n",
    "print(\"CONVERGENCIA:\", RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **2.- EJERCICIO 2:**\n",
    "\n",
    "---\n",
    "\n",
    "Programar las funciones que calcule el gradiente y la Hessiana usando el método\n",
    "de diferencias finitas.\n",
    "\n",
    "## **2.1.**\n",
    "Programe la función que calcule una aproximación del gradiente de una función\n",
    "   $f(\\mathbf{x})$ en un punto $\\mathbf{x}\\in\\mathbb{R}^n$ dado usando el esquema de \n",
    "   diferencias finitas hacia adelante    (Página 20 de la Clase 20).\n",
    "   \n",
    "- La función recibe como parámetros la función $f$, el punto $\\mathbf{x}$ y\n",
    "  el incremento $h$ y devuelve el arreglo de tamaño $n$ con las aproximaciones\n",
    "  de  aproximaciones de las derivadas parciales en el punto $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRADIENT(f, x: np.array, h: float):\n",
    "    \"\"\"\n",
    "    COMPUTE THE GRADIENT OF A FUNCTION USING FORWARD FINITE DIFFERENCES.\n",
    "\n",
    "    Args:\n",
    "    - f: function to compute the gradient.\n",
    "    - x: point to compute the gradient.\n",
    "    - h: step size.\n",
    "\n",
    "    Outputs:\n",
    "    - grad: gradient of f at x.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        ei = np.zeros(n)\n",
    "        ei[i] = 1\n",
    "        grad[i] = (f(x + h*ei) - f(x)) / h\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.**\n",
    "Programe la función que calcule una aproximación de la Hessiana de una función\n",
    "   $f(\\mathbf{x})$ en un punto $\\mathbf{x}\\in\\mathbb{R}^n$ dado usando el esquema de \n",
    "   diferencias finitas de la Página 22  de la Clase 20.\n",
    "\n",
    "- La función recibe como parámetros la función $f$, el punto $\\mathbf{x}$ y\n",
    "  el incremento $h$ y devuelve una matriz simétrica de tamaño $n$ que tiene\n",
    "  las aproximaciones de las segundas derivadas parciales de $f$  en el punto $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HESSIAN(f, x: np.array, h: float):\n",
    "    \"\"\"\n",
    "    COMPUTE THE HESSIAN MATRIX OF A FUNCTION USING FINITE DIFFERENCES.\n",
    "\n",
    "    Args:\n",
    "    - f: function to compute the Hessian.\n",
    "    - x: point to compute the Hessian.\n",
    "    - h: step size.\n",
    "\n",
    "    Outputs:\n",
    "    - Hess: Hessian of f at x.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    Hess = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        ei = np.zeros(n)\n",
    "        ei[i] = 1\n",
    "        for j in range(n):\n",
    "            ej = np.zeros(n)\n",
    "            ej[j] = 1\n",
    "            Hess[i,j] = (f(x + h*ei + h*ej) - f(x + h*ei) - f(x + h*ej) + f(x))/(h**2)\n",
    "    return Hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3.**\n",
    "Modifique la función `errorRelativo_grad`  para reportar estadísticas del\n",
    "   error relativo de la implementación del gradiente analítico `gradf` de \n",
    "   una función respecto al gradiente calculado con `autograd`, para que mida \n",
    "   el error relativo entre la función `gradf` y la aproximación del gradiente \n",
    "   usando diferencias finitas.\n",
    "   Hay que agregar como parámetro de `errorRelativo_grad` el incremento $h$\n",
    "   para que se pueda llamar la función del Punto 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorRelativo_grad(f, gradf, n: int, nt: int, h: float):\n",
    "    \"\"\"\n",
    "    Print statistics of relative errors in the gradient calculation (analytical vs. numerical)\n",
    "    using finite differences.\n",
    "    \n",
    "    Args:\n",
    "    - f:     function to find the gradient.\n",
    "    - gradf: analytical gradient of f.\n",
    "    - n:     dimension.\n",
    "    - nt:    number of tests.\n",
    "    - h:     step size.\n",
    "    \"\"\"\n",
    "    ve = np.zeros(nt)\n",
    "    gf = lambda x: GRADIENT(f = f, x = x, h = h) # Funcion gradiente generada con autograd.\n",
    "    for i in range(nt):\n",
    "        x0  = np.random.randn(n)\n",
    "        g0  = gradf(x0)\n",
    "        ga  = gf(x0)\n",
    "        ve[i] = np.linalg.norm(g0-ga) / np.linalg.norm(ga)\n",
    "    print('ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h =', h)\n",
    "    print('Min: %.2e   Media: %.2e    Max: %.2e' %(np.min(ve), np.mean(ve), np.max(ve)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4.**\n",
    "Programar la función `errorRelativo_hess`, similar a la función del punto anterior,\n",
    "   para que reporte estadísticas del error relativo entre una función que calcula\n",
    "   la Hessiana de $f$ de manera analítica en un punto  $\\mathbf{x}$ y la aproximación\n",
    "   de la Hessiana en  $\\mathbf{x}$ usando diferencias finitas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorRelativo_hess(f, Hessf, n: int , nt: int, h: float):\n",
    "    \"\"\"\n",
    "    Print statistics of relative errors in the Hessian matrix calculation (analytical vs. numerical)\n",
    "    using finite differences.\n",
    "    \n",
    "    Args:\n",
    "    - f:     function to find the Hessian.\n",
    "    - Hessf: analytical Hessian of f.\n",
    "    - n:     dimension.\n",
    "    - nt:    number of tests.\n",
    "    - h:     step size.\n",
    "    \"\"\"\n",
    "    ve = np.zeros(nt)\n",
    "    Hf = lambda x: HESSIAN(f = f, x = x, h = h) # Funcion Hessiana generada con autograd.\n",
    "    for i in range(nt):\n",
    "        x0  = np.random.randn(n)\n",
    "        H0  = Hessf(x0)\n",
    "        Ha  = Hf(x0)\n",
    "        ve[i] = np.linalg.norm(H0-Ha) / np.linalg.norm(Ha)\n",
    "    print('ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h =', h)\n",
    "    print('Min: %.2e   Media: %.2e    Max: %.2e' %(np.min(ve), np.mean(ve), np.max(ve)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.5.**\n",
    "Pruebe las funciones `errorRelativo_grad` con cada una de las funciones \n",
    "   del Ejercicio 1 usando $h=10^{-5}, 10^{-6}, 10^{-7}, 10^{-8}$.\n",
    "   ¿Cuál es el valor de $h$ que conviene usar para aproximar el gradiente y cuál\n",
    "   para aproximar la Hessiana?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **2.5.1. Función cuadrática 1:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "**$n=10$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-05\n",
      "Min: 2.90e-06   Media: 5.08e-06    Max: 9.30e-06 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-06\n",
      "Min: 2.74e-07   Media: 5.28e-07    Max: 9.88e-07 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-07\n",
      "Min: 1.60e-08   Media: 5.36e-08    Max: 1.20e-07 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-08\n",
      "Min: 1.39e-08   Media: 6.34e-08    Max: 2.39e-07 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "Hessf_cuad = lambda x: A1\n",
    "\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-5)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-6)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-7)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-05\n",
      "Min: 8.30e-06   Media: 3.39e-05    Max: 1.07e-04 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-06\n",
      "Min: 1.17e-03   Media: 3.54e-03    Max: 1.04e-02 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-07\n",
      "Min: 6.58e-02   Media: 3.22e-01    Max: 1.06e+00 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-08\n",
      "Min: 9.75e-01   Media: 1.01e+00    Max: 1.04e+00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 50, h = 1e-5)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 50, h = 1e-6)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 50, h = 1e-7)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$n=100$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-05\n",
      "Min: 4.21e-06   Media: 5.03e-06    Max: 5.80e-06 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-06\n",
      "Min: 3.89e-07   Media: 4.99e-07    Max: 5.82e-07 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-07\n",
      "Min: 3.66e-08   Media: 1.03e-07    Max: 2.95e-07 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-08\n",
      "Min: 2.74e-07   Media: 9.04e-07    Max: 2.93e-06 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "Hessf_cuad = lambda x: A1\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-5)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-6)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-7)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-05\n",
      "Min: 7.84e-04   Media: 1.10e-03    Max: 1.75e-03 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-06\n",
      "Min: 5.55e-02   Media: 1.40e-01    Max: 3.02e-01 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-07\n",
      "Min: 9.75e-01   Media: 9.94e-01    Max: 1.01e+00 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-08\n",
      "Min: 1.00e+00   Media: 1.00e+00    Max: 1.00e+00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 10, h = 1e-5)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 10, h = 1e-6)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 10, h = 1e-7)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 10, h = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **2.5.2. Función de cuadrática 2:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "**$n=10$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-05\n",
      "Min: 1.79e-06   Media: 3.51e-06    Max: 8.29e-06 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-06\n",
      "Min: 1.46e-07   Media: 4.03e-07    Max: 1.17e-06 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-07\n",
      "Min: 1.14e-08   Media: 3.56e-08    Max: 7.63e-08 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-08\n",
      "Min: 6.25e-09   Media: 4.86e-08    Max: 1.32e-07 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "Hessf_cuad = lambda x: A2\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-5)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-6)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-7)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-05\n",
      "Min: 6.59e-06   Media: 2.57e-05    Max: 9.33e-05 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-06\n",
      "Min: 1.67e-04   Media: 1.89e-03    Max: 6.14e-03 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-07\n",
      "Min: 3.18e-02   Media: 2.56e-01    Max: 1.05e+00 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-08\n",
      "Min: 9.39e-01   Media: 1.00e+00    Max: 1.04e+00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 50, h = 1e-5)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 50, h = 1e-6)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 50, h = 1e-7)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$n=100$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-05\n",
      "Min: 2.00e-06   Media: 2.74e-06    Max: 3.80e-06 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-06\n",
      "Min: 2.04e-07   Media: 2.75e-07    Max: 4.24e-07 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-07\n",
      "Min: 2.37e-08   Media: 4.72e-08    Max: 7.84e-08 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-08\n",
      "Min: 2.28e-07   Media: 3.71e-07    Max: 6.53e-07 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "Hessf_cuad = lambda x: A2\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-5)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-6)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-7)\n",
    "errorRelativo_grad(f = f_cuad, gradf = gradf_cuad, n = n, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-05\n",
      "Min: 3.87e-04   Media: 7.35e-04    Max: 1.04e-03 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-06\n",
      "Min: 2.94e-02   Media: 5.33e-02    Max: 8.04e-02 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-07\n",
      "Min: 9.30e-01   Media: 9.84e-01    Max: 1.02e+00 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-08\n",
      "Min: 9.99e-01   Media: 1.00e+00    Max: 1.00e+00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 10, h = 1e-5)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 10, h = 1e-6)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 10, h = 1e-7)\n",
    "errorRelativo_hess(f = f_cuad, Hessf = Hessf_cuad, n = n, nt = 10, h = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **2.5.3. Función de Beale :** Para $\\mathbf{x}=(x_1,x_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-05\n",
      "Min: 1.51e-06   Media: 7.84e-06    Max: 4.87e-05 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-06\n",
      "Min: 5.61e-08   Media: 6.87e-07    Max: 5.20e-06 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-07\n",
      "Min: 6.28e-09   Media: 1.23e-07    Max: 1.58e-06 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-08\n",
      "Min: 3.85e-09   Media: 2.13e-08    Max: 4.99e-08 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_grad(f = f_Beale, gradf = grad_Beale, n = 2, nt = 50, h = 1e-5)\n",
    "errorRelativo_grad(f = f_Beale, gradf = grad_Beale, n = 2, nt = 50, h = 1e-6)\n",
    "errorRelativo_grad(f = f_Beale, gradf = grad_Beale, n = 2, nt = 50, h = 1e-7)\n",
    "errorRelativo_grad(f = f_Beale, gradf = grad_Beale, n = 2, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-05\n",
      "Min: 2.83e-06   Media: 1.78e-05    Max: 5.69e-05 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-06\n",
      "Min: 3.03e-06   Media: 2.33e-04    Max: 1.34e-03 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-07\n",
      "Min: 2.09e-04   Media: 3.66e-02    Max: 1.58e-01 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-08\n",
      "Min: 6.64e-02   Media: inf    Max: inf \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/2wf1t3v96t99m5n9pzmlm9pm0000gn/T/ipykernel_2237/892321556.py:19: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  ve[i] = np.linalg.norm(H0-Ha) / np.linalg.norm(Ha)\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_hess(f = f_Beale, Hessf = Hess_Beale, n = 2, nt = 50, h = 1e-5)\n",
    "errorRelativo_hess(f = f_Beale, Hessf = Hess_Beale, n = 2, nt = 50, h = 1e-6)\n",
    "errorRelativo_hess(f = f_Beale, Hessf = Hess_Beale, n = 2, nt = 50, h = 1e-7)\n",
    "errorRelativo_hess(f = f_Beale, Hessf = Hess_Beale, n = 2, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **2.5.4. Función de Himmelblau:** Para $\\mathbf{x}=(x_1,x_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-05\n",
      "Min: 9.73e-07   Media: 7.60e-06    Max: 4.78e-05 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-06\n",
      "Min: 5.75e-08   Media: 6.74e-07    Max: 3.32e-06 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-07\n",
      "Min: 1.20e-08   Media: 5.76e-08    Max: 2.97e-07 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-08\n",
      "Min: 3.63e-09   Media: 1.47e-07    Max: 1.04e-06 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_grad(f = f_Himmelblau, gradf = grad_Himmelblau, n = 2, nt = 50, h = 1e-5)\n",
    "errorRelativo_grad(f = f_Himmelblau, gradf = grad_Himmelblau, n = 2, nt = 50, h = 1e-6)\n",
    "errorRelativo_grad(f = f_Himmelblau, gradf = grad_Himmelblau, n = 2, nt = 50, h = 1e-7)\n",
    "errorRelativo_grad(f = f_Himmelblau, gradf = grad_Himmelblau, n = 2, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-05\n",
      "Min: 5.49e-06   Media: 1.84e-05    Max: 4.12e-05 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-06\n",
      "Min: 2.32e-04   Media: 1.72e-03    Max: 4.09e-03 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-07\n",
      "Min: 1.07e-02   Media: 1.48e-01    Max: 3.36e-01 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-08\n",
      "Min: 9.50e-01   Media: inf    Max: inf \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/2wf1t3v96t99m5n9pzmlm9pm0000gn/T/ipykernel_2237/892321556.py:19: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  ve[i] = np.linalg.norm(H0-Ha) / np.linalg.norm(Ha)\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_hess(f = f_Himmelblau, Hessf = Hess_Himmelblau, n = 2, nt = 50, h = 1e-5)\n",
    "errorRelativo_hess(f = f_Himmelblau, Hessf = Hess_Himmelblau, n = 2, nt = 50, h = 1e-6)\n",
    "errorRelativo_hess(f = f_Himmelblau, Hessf = Hess_Himmelblau, n = 2, nt = 50, h = 1e-7)\n",
    "errorRelativo_hess(f = f_Himmelblau, Hessf = Hess_Himmelblau, n = 2, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **2.5.5. Función de Rosenbrock:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$n=2$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-05\n",
      "Min: 2.92e-06   Media: 3.71e-05    Max: 4.61e-04 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-06\n",
      "Min: 5.06e-07   Media: 1.83e-06    Max: 2.14e-05 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-07\n",
      "Min: 3.97e-08   Media: 2.38e-07    Max: 3.67e-06 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-08\n",
      "Min: 1.28e-09   Media: 2.29e-08    Max: 8.79e-08 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 2, nt = 50, h = 1e-5)\n",
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 2, nt = 50, h = 1e-6)\n",
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 2, nt = 50, h = 1e-7)\n",
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 2, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-05\n",
      "Min: 4.51e-06   Media: 1.78e-05    Max: 3.89e-05 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-06\n",
      "Min: 1.57e-06   Media: 7.20e-05    Max: 4.78e-04 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-07\n",
      "Min: 4.91e-05   Media: 8.10e-03    Max: 4.31e-02 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-08\n",
      "Min: 1.02e-02   Media: 4.83e-01    Max: 2.12e+00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 2, nt = 50, h = 1e-5)\n",
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 2, nt = 50, h = 1e-6)\n",
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 2, nt = 50, h = 1e-7)\n",
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 2, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$n=20$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-05\n",
      "Min: 4.87e-06   Media: 7.20e-06    Max: 1.25e-05 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-06\n",
      "Min: 4.63e-07   Media: 7.35e-07    Max: 1.19e-06 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-07\n",
      "Min: 4.32e-08   Media: 7.33e-08    Max: 1.28e-07 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-08\n",
      "Min: 2.26e-08   Media: 5.07e-08    Max: 9.84e-08 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 20, nt = 50, h = 1e-5)\n",
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 20, nt = 50, h = 1e-6)\n",
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 20, nt = 50, h = 1e-7)\n",
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 20, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-05\n",
      "Min: 1.10e-05   Media: 1.80e-05    Max: 3.70e-05 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-06\n",
      "Min: 3.35e-04   Media: 1.45e-03    Max: 2.95e-03 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-07\n",
      "Min: 3.27e-02   Media: 1.17e-01    Max: 3.13e-01 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-08\n",
      "Min: 9.58e-01   Media: 9.96e-01    Max: 1.04e+00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 20, nt = 50, h = 1e-5)\n",
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 20, nt = 50, h = 1e-6)\n",
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 20, nt = 50, h = 1e-7)\n",
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 20, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$n=40$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-05\n",
      "Min: 4.36e-06   Media: 6.98e-06    Max: 1.04e-05 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-06\n",
      "Min: 4.99e-07   Media: 6.98e-07    Max: 1.07e-06 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-07\n",
      "Min: 4.65e-08   Media: 6.85e-08    Max: 1.06e-07 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h = 1e-08\n",
      "Min: 5.35e-08   Media: 9.27e-08    Max: 1.35e-07 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 40, nt = 50, h = 1e-5)\n",
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 40, nt = 50, h = 1e-6)\n",
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 40, nt = 50, h = 1e-7)\n",
    "errorRelativo_grad(f = f_Rosenbrock, gradf = grad_Rosenbrock, n = 40, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-05\n",
      "Min: 1.53e-05   Media: 3.73e-05    Max: 7.27e-05 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-06\n",
      "Min: 9.82e-04   Media: 3.70e-03    Max: 1.04e-02 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-07\n",
      "Min: 1.51e-01   Media: 3.44e-01    Max: 5.80e-01 \n",
      "\n",
      "ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h = 1e-08\n",
      "Min: 9.88e-01   Media: 9.99e-01    Max: 1.00e+00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 40, nt = 50, h = 1e-5)\n",
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 40, nt = 50, h = 1e-6)\n",
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 40, nt = 50, h = 1e-7)\n",
    "errorRelativo_hess(f = f_Rosenbrock, Hessf = Hess_Rosenbrock, n = 40, nt = 50, h = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.6.**\n",
    "¿Cuál es el valor de $h$ que conviene usar para aproximar el gradiente y cuál\n",
    "   para aproximar la Hessiana?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Comparando los datos de error del cálculo del gradiente y de la matriz Hessiana de las funciones, se puede notar que para mayor exactitud, es conveniente elegir un valor pequeño de $h$ suficientemente pequeño, sin embargo, no demasiado ya que se nota un deterioro en el error promedio en el caso de $h=10^{-8}$. Además que existe más posibilidad de llegar a una indeterminación. Dado esto, para el cálculo del gradiente, el mejor valor de $h$ es $h=10^{-7}$.\n",
    ">\n",
    ">Por otro lado, para el cáclulo de la matriz Hessiana se puede notar que tiene un mejor desempeño cuando $h=10^{-5}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **3.- EJERCICIO 3:**\n",
    "\n",
    "---\n",
    "\n",
    "Seleccionar un artículo para el proyecto final.\n",
    "\n",
    "- El proyecto final se puede presentar de manera individual o en equipo \n",
    "  formado por dos estudiantes.\n",
    "- La entrega del proyecto consiste programar el algoritmo descrito \n",
    "  en el artículo seleccionado y realizar pruebas para reproducir algunos resultados\n",
    "  presentados en el artículo o diseñar los experimentos de prueba. El objetivo es\n",
    "  mostrar las ventajas o limitaciones que tiene el algoritmo propuesto.\n",
    "- Es válido delimitar el alcance, de manera que si aparecen varios algoritmos\n",
    "  en el artículo, se puede seleccionar alguno de ellos para su implementación y validación.\n",
    "- Hay que elaborar un reporte en el que se dé una introducción, \n",
    "  algunos fundamentos teóricos, el planteamiento del problema, la descripción del algoritmo, \n",
    "  los resultados obtenidos y las conclusiones.\n",
    "- Hay que hacer una presentación de unos 15 minutos en el día acordado y \n",
    "  entregar el reporte, el código y las pruebas realizadas.\n",
    "- Se puede entregar un notebook como el reporte y usarlo en la presentación,\n",
    "  para que no tener que elaborar un documento con el reporte, otro con el script \n",
    "  del código y pruebas y otro para la presentación.\n",
    "- Habrá dos fechas de entrega. La primera fecha es para los estudiantes de posgrado \n",
    "  que será entre el 27 de mayo y el 4 de junio. La segunda fecha es para los estudiantes \n",
    "  de licenciatura que será entre el 3 de junio y el 10 de junio.\n",
    "- Si el equipo está formado por un estudiante de licenciatura y otro de posgrado\n",
    "  tendrá que presentar el proyecto en la primera fecha.\n",
    "- Para la selección se puede tomar uno de los artículos de la lista \n",
    "  que se presenta a continuación. \n",
    "- Estos artículos son una referencia. También pueden proponer algún artículo adicional,\n",
    "  pero recomienda que cuiden que para entenderlo no tengan que revisar otras fuentes\n",
    "  o que tengan que implementar algoritmos que requieran de temas que no fueron cubiertos\n",
    "  en el curso y que les consuma demasiado tiempo hacer esa revisión, por ejemplo, \n",
    "  en temas de optimización combinatoria, entera, mixta, multiobjetivo, etc.\n",
    "\n",
    "1. Escriba el nombre de los miembros del equipo junto con el nombre del programa académico.\n",
    "2. Escriba el título del artículo seleccionado\n",
    "3. Si no es un artículo de la lista o que esté en el Classroom, agregue el PDF\n",
    "   como parte de la entrega de la Tarea 7.\n",
    "   \n",
    "------\n",
    "\n",
    "### Lista de artículos\n",
    "\n",
    "1. An improvement of the Goldstein line search. <br>\n",
    "   Arnold NeumaierMorteza Kimiaei. 2023.<br>\n",
    "   https://optimization-online.org/2022/11/an-efficient-gradient-free-line-search/\n",
    "2. A harmonic framework for stepsize selection in gradient methods.<br>\n",
    "   Giulia FerrandiMichiel E. HochstenbachNataša Krejić. 2022.<br>\n",
    "   https://optimization-online.org/2022/02/8803/\n",
    "3. An Adaptive Trust-Region Method Without Function Evaluations.<br>\n",
    "   Geovani Nunes GrapigliaGabriel Stella. 2022.<br>\n",
    "   https://optimization-online.org/2022/02/8787/\n",
    "4. Accelerated Scaled Memory-less SR1 method for Unconstrained Optimization.<br>\n",
    "   Neculai Andrei. 2021.<br>\n",
    "   https://camo.ici.ro/neculai/XXITR5.pdf\n",
    "5. Secant Penalized BFGS: A Noise Robust Quasi-Newton Method Via Penalizing The Secant Condition.<br>\n",
    "   Brian Irwin, Eldad Haber. <br>\n",
    "   https://arxiv.org/abs/2010.01275\n",
    "6. Regularized Step Directions in Nonlinear Conjugate Gradient Methods.<br>\n",
    "   Cassidy K. Buhler, Hande Y. Benson, David F. Shanno. 2023.<br>\n",
    "   https://arxiv.org/abs/2110.06308\n",
    "7. A Levenberg-Marquardt Method for Nonsmooth Regularized Least Squares.<br>\n",
    "   Aleksandr Y. Aravkin, Robert Baraldi, Dominique Orban. 2023.<br>\n",
    "   https://arxiv.org/abs/2301.02347\n",
    "8. Globally linearly convergent nonlinear conjugate gradients without Wolfe line search.<br>\n",
    "   Arnold NeumaierMorteza KimiaeiBehzad Azmi. 2023.<br>\n",
    "   https://optimization-online.org/2022/12/globally-linearly-convergent-nonlinear-conjugate-gradients-without-wolfe-line-search/\n",
    "9. A nonlinear conjugate gradient method with complexity guarantees and its application to nonconvex regression.<br>\n",
    "   Rémi Chan--Renous-Legoubin, Clément W. Royer. 2022.<br>\n",
    "   https://arxiv.org/abs/2201.08568\n",
    "10. Iteration Complexity of Fixed-Step Methods by Nesterov and Polyak for Convex Quadratic Functions.<br>\n",
    "    Melinda Hagedorn, Florian Jarre. 2022. <br>\n",
    "    https://arxiv.org/abs/2211.10234\n",
    "11. Subsampled cubic regularization method for finite-sum minimization.<br> \n",
    "    Max L. N. Gonçalves. 2022.<br>\n",
    "    https://files.cercomp.ufg.br/weby/up/922/o/Inexact_CNMSubsampled16November2022.pdf\n",
    "12. Optimized convergence of stochastic gradient descent by weighted averaging.<br>\n",
    "    Melinda Hagedorn, Florian Jarre. 2022. <br>\n",
    "    https://arxiv.org/abs/2209.14092\n",
    "13. Two efficient gradient methods with approximately optimal stepsizes based on regularization models for unconstrained optimization. <br>\n",
    "    Zexian Liu, Wangli Chu, Hongwei Liu. <br>\n",
    "    https://arxiv.org/abs/1907.01794\n",
    "14. A Trust Region Method for the Optimization of Noisy Functions.<br>\n",
    "    Shigeng Sun, Jorge Nocedal. 2022.<br>\n",
    "    https://arxiv.org/abs/2201.00973\n",
    "15. A modified quasi-Newton method for nonlinear equations. <br>\n",
    "    Xiaowei Fang, Qin Ni, Meilan Zeng. 2018. <br>\n",
    "    https://drive.google.com/file/d/13s8ey-LVioDjx3BFksWTUzccRXV9sEbG/view?usp=sharing\n",
    "16. A minibatch stochastic Quasi-Newton method adapted for nonconvex deep learning problems. <br>\n",
    "    Joshua D. GriffinMajid JahaniMartin TakacSeyedalireza YektamaramWenwen Zhou. 2022. <br>\n",
    "    https://optimization-online.org/2022/01/8760/\n",
    "17. Nonlinear conjugate gradient for smooth convex functions. <br>\n",
    "    Sahar Karimi, Stephen Vavasis. 2024.\n",
    "    https://arxiv.org/abs/2111.11613\n",
    "18. Quadratic Regularization Methods with Finite-Difference Gradient Approximations. <br>\n",
    "    Geovani Nunes Grapiglia. 2021. <br>\n",
    "    https://optimization-online.org/wp-content/uploads/2021/11/8665.pdf\n",
    "19. Adaptive Finite-Difference Interval Estimation for Noisy Derivative-Free Optimization. <br>\n",
    "    Hao-Jun Michael Shi, Yuchen Xie, Melody Qiming Xuan, Jorge Nocedal. 2021.<br>\n",
    "    https://arxiv.org/abs/2110.06380\n",
    "20. Full-low evaluation methods for derivative-free optimization. <br>\n",
    "    Albert S. Berahas, Luis Nunes, Vicente Oumaima Sohab. 2021.\n",
    "    https://arxiv.org/abs/2107.11908\n",
    "21. A stochastic first-order trust-region method with inexact restoration for finite-sum minimization.<br>\n",
    "    Stefania Bellavia, Natasa Krejic, Benedetta Morini, Simone Rebegoldi. 2022.<br>\n",
    "    https://arxiv.org/abs/2107.03129\n",
    "22. Robust Conjugate Gradient Methods for Non-smooth Convex Optimization and Image Processing Problems.<br>\n",
    "    Salar Farahmand-Tabar, Fahimeh Abdollahi, and Masoud Fatemi. <br>\n",
    "    https://drive.google.com/file/d/1ItSZ_7N0QKJLvqI8jhyAKU532oxWMmwg/view?usp=sharing\n",
    "23. A family of hybrid conjugate gradient method with restart procedure for\n",
    "    unconstrained optimizations and image restorations.<br>\n",
    "    Xianzhen Jiang, Xiaomin Ye, Zefeng Huang, Meixing Liu. 2023<br>\n",
    "    https://drive.google.com/file/d/1h1TQpydDxkYBF0G_jd7O7-9J69X-V-dI/view?usp=sharing\n",
    "24. A modified Broyden-like quasi-Newton method for nonlinear equations. <br>\n",
    "    Weijun Zhou, Li Zhang. 2020. <br>\n",
    "    https://drive.google.com/file/d/152StQd3SVdNUXRSSboABFpjHbqEVxUP9/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

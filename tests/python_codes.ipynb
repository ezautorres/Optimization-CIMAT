{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se tienen los códigos rápidos de las tareas que se pueden utilizar. Se dividen por temas y según el uso en las tareas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CÓDIGOS VARIOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BACKTRAKING(alpha_i: float, p: float, c: float, \n",
    "                xk: np.array, f, fxk: np.array,\n",
    "                gradfxk: np.array, pk: np.array, Nb: int):\n",
    "    alpha = alpha_i\n",
    "    for i in range(Nb):\n",
    "        if f(xk + alpha*pk) <= fxk + c*alpha*(gradfxk.T)@pk:\n",
    "            return alpha, i\n",
    "        alpha = p*alpha\n",
    "    return alpha, Nb\n",
    "\n",
    "def contornosFnc2D(fncf, xleft, xright, ybottom, ytop, levels,\n",
    "                    secuencia1 = None, secuencia2 = None):\n",
    "    ax = np.linspace(xleft, xright, 250)\n",
    "    ay = np.linspace(ybottom, ytop, 200)\n",
    "    mX, mY = np.meshgrid(ax, ay)\n",
    "    mZ = mX.copy()\n",
    "    for i,y in enumerate(ay):\n",
    "        for j,x in enumerate(ax):\n",
    "            mZ[i,j] = fncf(np.array([x,y]))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    CS = ax.contour(mX, mY, mZ, levels, cmap='brg_r')\n",
    "    if secuencia1 is not None:\n",
    "        puntos1 = np.array(secuencia1)\n",
    "        ax.plot(puntos1[:,0], puntos1[:,1], marker = '.',\n",
    "                    color='#000000', label = r\"$\\Delta_{max} = 4$\")\n",
    "    if secuencia2 is not None:\n",
    "        puntos2 = np.array(secuencia2)\n",
    "        ax.plot(puntos2[:,0], puntos2[:,1], marker = '.', markersize = 3,\n",
    "                    color='#FF0000', label = r\"$\\Delta_{max} = 0.25$\")\n",
    "    ax.plot()\n",
    "    ax.grid(True)\n",
    "    cbar = fig.colorbar(CS)\n",
    "    return ax\n",
    "\n",
    "def BACKTRAKING_WOLF(a_init: float, p: float, c1: float,\n",
    "                c2: float, xk: np.array, f, gradf,\n",
    "                dk: np.array, Nb: int):\n",
    "    a = a_init\n",
    "    for i in range(Nb): # STRONG / NORMAL\n",
    "        #if (f(xk + a*dk) <= f(xk) + c1*a*gradf(xk).T @ dk) and (np.abs(gradf(xk + a*dk).T @ dk) <= -c2*gradf(xk).T @ dk):\n",
    "        if (f(xk + a*dk) <= f(xk) + c1*a*gradf(xk).T @ dk) and (gradf(xk + a*dk).T @ dk >= c2*gradf(xk).T @ dk):\n",
    "            return a, i\n",
    "        a = p*a\n",
    "    return a, Nb\n",
    "\n",
    "def DESC_MAX_BACKTRACKING(f, gradf, xk: np.array, tol: float,\n",
    "                        max_iter: int, alpha_i: float,\n",
    "                        p: float, c: float, Nb: int):\n",
    "    \"\"\"\n",
    "    Gradient Descent with Backtracking Line Search.\n",
    "    \n",
    "    Args:\n",
    "    - f:        function to minimize.\n",
    "    - gradf:    gradient of f.\n",
    "    - xk:       initial point.\n",
    "    - tol:      tolerance.\n",
    "    - max_iter: maximum number of iterations.\n",
    "    - alpha_i:  initial value of alpha.\n",
    "    - p:        factor to reduce alpha.\n",
    "    - c:        factor to reduce alpha.\n",
    "    - Nb:       maximum number of iterations for backtracking.\n",
    "\n",
    "    Returns:\n",
    "    - xk:  optimal point.\n",
    "    - k:   number of iterations.\n",
    "    - T/F: if the algorithm converged, False otherwise.\n",
    "    \"\"\"\n",
    "    for k in range(max_iter):\n",
    "        gk = gradf(xk)\n",
    "        pk = -gk\n",
    "        ak = BACKTRAKING(alpha_i, p, c, xk, f, f(xk), gk, pk, Nb)[0]\n",
    "        if ak*np.linalg.norm(gk) < tol:\n",
    "            return xk, k, True\n",
    "        xk = xk + ak * pk\n",
    "    return xk, max_iter, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FUNCIONES PRUEBA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_Himmelblau(x: np.array):\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "def grad_Himmelblau(x: np.array):\n",
    "    x1 = 4*x[0]*(x[0]**2 + x[1] - 11) + 2*(x[0] + x[1]**2 - 7)\n",
    "    x2 = 2*(x[0]**2 + x[1] - 11) + 4*x[1]*(x[0] + x[1]**2 - 7)\n",
    "    return np.array([x1,x2], dtype = float)\n",
    "def Hess_Himmelblau(x: np.array):\n",
    "    x11 = 12*x[0]**2 + 4*x[1] - 42\n",
    "    x12 = 4*x[0] + 4*x[1]\n",
    "    x22 = 4*x[0] + 12*x[1]**2 - 26\n",
    "    return np.array([[x11, x12], [x12, x22]], dtype = float)\n",
    "\n",
    "def f_Beale(x: np.array):\n",
    "    return (1.5 - x[0] + x[0]*x[1])**2 + (2.25 - x[0] + x[0]*x[1]**2)**2 + (2.625 - x[0] + x[0]*x[1]**3)**2\n",
    "def grad_Beale(x: np.array):\n",
    "    x1 = 2*(x[1] - 1)*(1.5 - x[0] + x[0]*x[1]) + 2*(x[1]**2 - 1)*(2.25 - x[0] + x[0]*x[1]**2) + 2*(x[1]**3 - 1)*(2.625 - x[0] + x[0]*x[1]**3)\n",
    "    x2 = 2*x[0]*(1.5 - x[0] + x[0]*x[1]) + 4*x[0]*x[1]*(2.25 - x[0] + x[0]*x[1]**2) + 6*x[0]*(x[1]**2)*(2.625 - x[0] + x[0]*x[1]**3)\n",
    "    return np.array([x1,x2], dtype = float)\n",
    "def Hess_Beale(x: np.array):\n",
    "    x11 = 2*(x[1]**3 - 1)**2 + 2*(x[1]**2 - 1)**2 + 2*(x[1] - 1)**2\n",
    "    x12 = 4*x[0]*x[1]*(x[1]**2 - 1) + 4*x[1]*(x[0]*x[1]**2 - x[0]+2.25) + 6*x[0]*x[1]**2*(x[1]**3 - 1) + 6*x[1]**2*(x[0]*x[1]**3 - x[0]+2.625) + 2*x[0]*(x[1]-1) + 2*(x[0]*x[1] - x[0]+1.5)\n",
    "    x22 = 18*x[0]**2*x[1]**4 + 8*x[0]**2*x[1]**2 + 2*x[0]**2 + 12*x[0]*x[1]*(x[0]*x[1]**3 - x[0] + 2.625) + 4*x[0]*(x[0]*x[1]**2 - x[0]+2.25)\n",
    "    return np.array([[x11, x12], [x12, x22]], dtype = float)\n",
    "\n",
    "def f_Rosenbrock(x: np.array):\n",
    "    n = len(x)\n",
    "    s = 0\n",
    "    for i in range(n-1):\n",
    "        s = s + 100*(x[i+1] - x[i]**2)**2 + (1 - x[i])**2\n",
    "    return s\n",
    "def grad_Rosenbrock(x: np.array):\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    grad[0] = -400*x[0]*(x[1] - x[0]**2) - 2*(1-x[0])\n",
    "    grad[n-1] = 200*(x[n-1] - x[n-2]**2)\n",
    "    for j in range(1,n-1):\n",
    "        grad[j] = 200*(x[j]-x[j-1]**2) - 400*x[j]*(x[j+1] - x[j]**2) - 2*(1-x[j])\n",
    "    return np.array(grad, dtype = float)\n",
    "def Hess_Rosenbrock(x: np.array):\n",
    "    n = len(x)\n",
    "    Hess = np.zeros((n,n))\n",
    "    Hess[0,0] = -400*(x[1]-x[0]**2) + 800*x[0]**2 + 2\n",
    "    Hess[1,0] = -400*x[0]\n",
    "    Hess[n-2,n-1] = -400*x[n-2]\n",
    "    Hess[n-1,n-1] = 200\n",
    "    for j in range(1,n-1):\n",
    "        Hess[j-1,j] = -400*x[j-1]\n",
    "        Hess[j,j] = -400*(x[j+1]-x[j]**2) +800*x[j]**2 + 202\n",
    "        Hess[j+1,j] = -400*x[j]\n",
    "    return np.array(Hess, dtype = float)\n",
    "\n",
    "n = 10\n",
    "A1 = n*np.eye(n, dtype = float) + np.ones([n,n], dtype = float)\n",
    "b1 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A1 @ x - b1.T @ x\n",
    "gradf_cuad = lambda x: A1 @ x - b1\n",
    "Hessf_cuad = lambda x: A1\n",
    "\n",
    "n = 10\n",
    "A2 = np.zeros((n,n), dtype = float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A2[i,j] = np.exp(-0.25*(i-j)**2)\n",
    "b2 = np.ones(n, dtype = float)\n",
    "f_cuad = lambda x: 0.5 * x.T @ A2 @ x - b2.T @ x\n",
    "gradf_cuad = lambda x: A2 @ x - b2\n",
    "Hessf_cuad = lambda x: A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array([1.0, 1.2, 3.0, 3.2], dtype = float)\n",
    "A = np.array(\n",
    "    [[10, 3, 17, 3.5, 1.7, 8],\n",
    "    [0.05, 10, 17, 0.1, 8, 14],\n",
    "    [3, 3.5, 1.7, 10, 17, 8],\n",
    "    [17, 8, 0.05, 10, 0.1, 14]], dtype = float)\n",
    "P = 10**(-4) * np.array(\n",
    "    [[1312, 1696, 5569, 124, 8283, 5886],\n",
    "    [2329, 4135, 8307, 3736, 1004, 9991],\n",
    "    [2348, 1451, 3522, 2883, 3047, 6650],\n",
    "    [4047, 8828, 8732, 5743, 1091, 381]], dtype = float)\n",
    "def f_Hartman(x: np.array):\n",
    "    result = 0\n",
    "    for i in range(4):\n",
    "        inner_sum = 0\n",
    "        for j in range(6):\n",
    "            inner_sum += A[i][j] * (x[j] - P[i][j])**2\n",
    "        result += alpha[i] * np.exp(-inner_sum)\n",
    "    return -(2.58 + result)/1.94\n",
    "def grad_Hartman(x: np.array):\n",
    "    grad = np.zeros(6)\n",
    "    for k in range(6):\n",
    "        for i in range(4):\n",
    "            inner_sum = 0\n",
    "            for j in range(6):\n",
    "                inner_sum += A[i][j] * (x[j] - P[i][j])**2\n",
    "            grad[k] += alpha[i] * A[i][k] * (x[k] - P[i][k]) * np.exp(-inner_sum)\n",
    "    return np.array(2*grad/1.94, dtype = float)\n",
    "def Hess_Hartman(x: np.array):\n",
    "    hess = np.zeros((6, 6))\n",
    "    for l in range(6):\n",
    "        for k in range(6):\n",
    "            for i in range(4):\n",
    "                inner_sum = 0\n",
    "                for j in range(6):\n",
    "                    inner_sum += A[i][j] * (x[j] - P[i][j])**2\n",
    "                if k != l:\n",
    "                    hess[l][k] += -2*alpha[i] * A[i][k] * A[i][l] * (x[k]-P[i][k])*(x[l]-P[i][l]) * np.exp(-inner_sum)\n",
    "                else:\n",
    "                    hess[l][l] += alpha[i] * A[i][l] * (1-2*A[i][l]*(x[l]-P[i][l])**2) * np.exp(-inner_sum)\n",
    "    return np.array(2*hess/1.94, dtype = float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNIDAD 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REG_CONF_CAUCHY(f, gradf, Hessf, xk: np.array,\n",
    "                    maxiter: int, tol: float, Dmax: float,\n",
    "                    Dmin: float, eta: float):\n",
    "    \"\"\"\n",
    "    Confident region method with Cauchy step.\n",
    "    Args:\n",
    "    - f: function to minimize.\n",
    "    - gradf: gradient of f.\n",
    "    - Hessf: Hessian of f.\n",
    "    - xk: initial point.\n",
    "    - maxiter: maximum number of iterations.\n",
    "    - tol: tolerance.\n",
    "    - Dmax: maximum trust region radius.\n",
    "    - Dmin: minimum trust region radius.\n",
    "    - eta: parameter for the acceptance of the step.\n",
    "    Returns:\n",
    "    - k: number of iterations.\n",
    "    - xk: final point.\n",
    "    - gk: gradient at xk.\n",
    "    - True if the method converged, False otherwise.\n",
    "    \"\"\"\n",
    "    M = None\n",
    "    if len(xk)==2:\n",
    "        M = [xk]\n",
    "    em = np.finfo(float).eps\n",
    "    Dk = (Dmax + Dmin)/4\n",
    "    for k in range(maxiter):\n",
    "        gk = gradf(xk)\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            return k, xk, gk, True, M\n",
    "        Bk = Hessf(xk)\n",
    "        if gk.T @ Bk @ gk <= em:\n",
    "            tk = 1.0\n",
    "        else:\n",
    "            tk = min(1.0, np.linalg.norm(gk)**3/(Dk*gk.T @ Bk @ gk))\n",
    "        pkC = -tk*(Dk/np.linalg.norm(gk))*gk\n",
    "        pk = (f(xk) - f(xk+pkC)) / (- (pkC.T @ gk + 0.5 * pkC.T @ Bk @ pkC)) # Cambio analitico\n",
    "        if pk < 0.25 and Dk > 4*Dmin:\n",
    "            Dk = Dk/4\n",
    "        elif pk > 0.75 and np.abs(np.linalg.norm(pk) - Dk) < em:\n",
    "            Dk = min(Dmax, 2*Dk)\n",
    "        else:\n",
    "            Dk = Dk\n",
    "        if pk > eta:\n",
    "            xk = xk + pkC\n",
    "        else:\n",
    "            xk = xk\n",
    "        if len(xk)==2:\n",
    "            M.append(xk)\n",
    "    return maxiter, xk, gk, False, M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNIDAD 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LINEAR_CONJUGATE_GRADIENT(xk: np.array, A: np.array,\n",
    "                        b: np.array, maxiter: int, tol: float):\n",
    "    \"\"\"\n",
    "    SOLVE THE SYSTEM OF EQUATIONS Ax = b, WHERE A IS A POSITIVE DEFINITE SYMMETRIC MATRIX USING THE CONJUGATE GRADIENT METHOD\n",
    "\n",
    "    Args:\n",
    "    - xk:      initial guess.\n",
    "    - A:       positive definite symmetric matrix.\n",
    "    - b:       vector b.\n",
    "    - maxiter: maximum number of iterations.\n",
    "    - tol:     tolerance.\n",
    "\n",
    "    Outputs:\n",
    "    - xk:  approach to the solution.\n",
    "    - rk:  residual.\n",
    "    - k:   number of iterations.\n",
    "    - T/F: if the method converged.\n",
    "    \"\"\"\n",
    "    rk = A @ xk - b\n",
    "    pk = - rk\n",
    "    for k in range(maxiter+1):\n",
    "        if np.linalg.norm(rk) < tol:\n",
    "            return xk, rk, k, True\n",
    "        ak = (rk.T @ rk) / (pk.T @ A @ pk)\n",
    "        xk = xk + ak * pk\n",
    "        rk_new = rk + ak * A @ pk\n",
    "        bk = (rk_new.T @ rk_new) / (rk.T @ rk)\n",
    "        pk = - rk_new + bk * pk\n",
    "        rk = rk_new\n",
    "    return xk, rk, maxiter, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NONLINEAR_CONJUGATE_GRADIENT_FR(xk: np.array, f, gradf, maxiter: int,\n",
    "                                tol: float, a_init: float, p: float,\n",
    "                                c1: float, c2: float, Nb: int):\n",
    "    \"\"\"\n",
    "    THIS FINDS THE MINIMIZER OF f USING THE NON-LINEAR CONJUGATE GRADIENT METHOD WITH THE FLETCHER-REEVES FORMULA.\n",
    "\n",
    "    Args:\n",
    "    - xk:      initial guess.\n",
    "    - f:       function to minimize.\n",
    "    - gradf:   gradient of the function to minimize.\n",
    "    - maxiter: maximum number of iterations.\n",
    "    - tol:     method tolerance.\n",
    "    - a_init:  initial value for the step size in backtracking.\n",
    "    - p:       reduction factor for the step size in backtracking.\n",
    "    - c1:      parameter for the sufficient descent condition.\n",
    "    - c2:      parameter for the curvature condition.\n",
    "    - Nb:      maximum number of iterations in backtracking.\n",
    "    \n",
    "    Outputs:\n",
    "    - xk:  approach to the minimizer of f.\n",
    "    - gk:  gradient of f at xk.\n",
    "    - k:   number of iterations.\n",
    "    - T/F: if the method converged.\n",
    "    - nr:  number of restarts (bk = 0).\n",
    "    \"\"\"\n",
    "    gk = gradf(xk)\n",
    "    dk = -gk\n",
    "    nr = 0\n",
    "    for k in range(maxiter + 1):\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            return xk, gk, k, True, nr\n",
    "        ak, k1 = BACKTRAKING_WOLF(a_init = a_init, p = p, c1 = c1, c2 = c2,\n",
    "                                xk = xk, f = f, gradf = gradf, dk = dk, Nb = Nb)\n",
    "        xk = xk + ak*dk\n",
    "        gk_n = gradf(xk)\n",
    "        if np.abs(gk_n.T @ gk) < 0.2*np.linalg.norm(gk_n)**2:\n",
    "            bk = (gk_n.T @ gk_n) / (gk.T @ gk)\n",
    "        else:\n",
    "            bk = 0\n",
    "            nr += 1\n",
    "        dk = -gk_n + bk*dk\n",
    "        gk = gk_n\n",
    "    return xk, gk, maxiter, False, nr\n",
    "\n",
    "def NONLINEAR_CONJUGATE_GRADIENT_HS(xk: np.array, f, gradf, maxiter: int,\n",
    "                                tol: float, a_init: float, p: float,\n",
    "                                c1: float, c2: float, Nb: int):\n",
    "    \"\"\"\n",
    "    THIS FINDS THE MINIMIZER OF f USING THE NON-LINEAR CONJUGATE GRADIENT METHOD WITH THE HESTENES-STIEFEL FORMULA.\n",
    "\n",
    "    Args:\n",
    "    - xk:      initial guess.\n",
    "    - f:       function to minimize.\n",
    "    - gradf:   gradient of the function to minimize.\n",
    "    - maxiter: maximum number of iterations.\n",
    "    - tol:     method tolerance.\n",
    "    - a_init:  initial value for the step size in backtracking.\n",
    "    - p:       reduction factor for the step size in backtracking.\n",
    "    - c1:      parameter for the sufficient descent condition.\n",
    "    - c2:      parameter for the curvature condition.\n",
    "    - Nb:      maximum number of iterations in backtracking.\n",
    "    \n",
    "    Outputs:\n",
    "    - xk:  approach to the minimizer of f.\n",
    "    - gk:  gradient of f at xk.\n",
    "    - k:   number of iterations.\n",
    "    - T/F: if the method converged.\n",
    "    - nr:  number of restarts (bk = 0).\n",
    "    \"\"\"\n",
    "    gk = gradf(xk)\n",
    "    dk = -gk\n",
    "    nr = 0\n",
    "    for k in range(maxiter + 1):\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            return xk, gk, k, True, nr\n",
    "        ak, k1 = BACKTRAKING_WOLF(a_init = a_init, p = p, c1 = c1, c2 = c2,\n",
    "                                xk = xk, f = f, gradf = gradf, dk = dk, Nb = Nb)\n",
    "        xk = xk + ak*dk\n",
    "        gk_n = gradf(xk)\n",
    "        yk = gk_n - gk\n",
    "        if np.abs(gk_n.T @ gk) < 0.2*np.linalg.norm(gk_n)**2:\n",
    "            bk = (gk_n.T @ yk) / (dk.T @ yk)\n",
    "        else:\n",
    "            bk = 0\n",
    "            nr += 1\n",
    "        dk = -gk_n + bk*dk\n",
    "        gk = gk_n\n",
    "    return xk, gk, maxiter, False, nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNIDAD 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRADIENT(f, x: np.array, h: float):\n",
    "    \"\"\"\n",
    "    COMPUTE THE GRADIENT OF A FUNCTION USING FORWARD FINITE DIFFERENCES.\n",
    "\n",
    "    Args:\n",
    "    - f: function to compute the gradient.\n",
    "    - x: point to compute the gradient.\n",
    "    - h: step size.\n",
    "\n",
    "    Outputs:\n",
    "    - grad: gradient of f at x.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        ei = np.zeros(n)\n",
    "        ei[i] = 1\n",
    "        grad[i] = (f(x + h*ei) - f(x)) / h\n",
    "    return grad\n",
    "\n",
    "def CG(gk: np.array, Hk: np.array, n: int, tol: float):\n",
    "    \"\"\"\n",
    "    CONJUGATE GRADIENT APPLIED TO NEWTON SYSTEM.\n",
    "\n",
    "    Args:\n",
    "    - gk:  gradient of f at xk.\n",
    "    - Hk:  Hessian of f at xk.\n",
    "    - n:   dimention.\n",
    "    - tol: tolerance.\n",
    "\n",
    "    Outputs:\n",
    "    - pk: search direction.\n",
    "    - j:  number of iterations.\n",
    "    \"\"\"\n",
    "    zj = 0\n",
    "    rj = gk\n",
    "    dj = -rj\n",
    "    for j in range(n-1):\n",
    "        if dj.T @ Hk @ dj <= 0:\n",
    "            if j == 0:\n",
    "                pk = -gk\n",
    "            else:\n",
    "                pk = zj\n",
    "        aj = (rj.T @ rj) / (dj.T @ Hk @ dj)\n",
    "        zj = zj + aj * dj\n",
    "        rj_n = rj + aj * Hk @ dj\n",
    "        Bj = (rj_n.T @ rj_n) / (rj.T @ rj)\n",
    "        dj = -rj_n + Bj * dj\n",
    "        if np.linalg.norm(rj_n) < tol:\n",
    "            pk = zj\n",
    "            return pk, j\n",
    "        rj = rj_n\n",
    "    pk = zj\n",
    "    return pk, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HESSIAN(f, x: np.array, h: float):\n",
    "    \"\"\"\n",
    "    COMPUTE THE HESSIAN MATRIX OF A FUNCTION USING FINITE DIFFERENCES.\n",
    "\n",
    "    Args:\n",
    "    - f: function to compute the Hessian.\n",
    "    - x: point to compute the Hessian.\n",
    "    - h: step size.\n",
    "\n",
    "    Outputs:\n",
    "    - Hess: Hessian of f at x.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    Hess = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        ei = np.zeros(n)\n",
    "        ei[i] = 1\n",
    "        for j in range(n):\n",
    "            ej = np.zeros(n)\n",
    "            ej[j] = 1\n",
    "            Hess[i,j] = (f(x + h*ei + h*ej) - f(x + h*ei) - f(x + h*ej) + f(x))/(h**2)\n",
    "    return Hess\n",
    "\n",
    "def NEWTON_CG_LINESEARCH_METHOD(f, gradf, Hessf, xk: np.array, tol: float, maxiter: int,\n",
    "                                alpha_i: float, p: float, c: float, Nb: int):\n",
    "    \"\"\"\n",
    "    NEWTON CONJUGATE GRADIENT METHOD WITH LINE SEARCH.\n",
    "\n",
    "    Args:\n",
    "    - f:       function to minimize.\n",
    "    - gradf:   gradient of f.\n",
    "    - Hessf:   Hessian of f.\n",
    "    - xk:      initial point.\n",
    "    - tol:     tolerance.\n",
    "    - maxiter: maximum number of iterations.\n",
    "    - alpha_i: initial alpha.\n",
    "    - p:       reduction factor.\n",
    "    - c:       constant for Armijo condition.\n",
    "    - Nb:      maximum number of iterations for line search.\n",
    "\n",
    "    Outputs:\n",
    "    - xk:   optimal point.\n",
    "    - gk:   gradient at xk.\n",
    "    - k:    number of iterations.\n",
    "    - T/F:  if the method converged.\n",
    "    - MEAN: average number of iterations for line search.\n",
    "    \"\"\"\n",
    "    n = len(xk)\n",
    "    iteraciones = []\n",
    "    for k in range(maxiter):\n",
    "        gk = gradf(xk)\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            return xk, gk, k, True, np.mean(iteraciones)\n",
    "        Hk = Hessf(xk)\n",
    "        ek = min(0.5, np.sqrt(np.linalg.norm(gk)))*np.sqrt(np.linalg.norm(gk))\n",
    "        pk, i = CG(gk, Hk, n, ek)\n",
    "        ak = BACKTRAKING(alpha_i = alpha_i, p = p, c = c, xk = xk, f = f,\n",
    "                            fxk=f(xk), gradfxk = gk, pk = pk, Nb = Nb)[0]\n",
    "        iteraciones.append(i)\n",
    "        xk = xk + ak * pk\n",
    "    return xk, gk, maxiter, False, np.mean(iteraciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorRelativo_grad(f, gradf, n: int, nt: int, h: float):\n",
    "    \"\"\"\n",
    "    Print statistics of relative errors in the gradient calculation (analytical vs. numerical)\n",
    "    using finite differences.\n",
    "    \n",
    "    Args:\n",
    "    - f:     function to find the gradient.\n",
    "    - gradf: analytical gradient of f.\n",
    "    - n:     dimension.\n",
    "    - nt:    number of tests.\n",
    "    - h:     step size.\n",
    "    \"\"\"\n",
    "    ve = np.zeros(nt)\n",
    "    gf = lambda x: GRADIENT(f = f, x = x, h = h) # Funcion gradiente generada con autograd.\n",
    "    for i in range(nt):\n",
    "        x0  = np.random.randn(n)\n",
    "        g0  = gradf(x0)\n",
    "        ga  = gf(x0)\n",
    "        ve[i] = np.linalg.norm(g0-ga) / np.linalg.norm(ga)\n",
    "    print('ERRORES RELATIVOS EN EL CÁLCULO DEL GRADIENTE PARA h =', h)\n",
    "    print('Min: %.2e   Media: %.2e    Max: %.2e' %(np.min(ve), np.mean(ve), np.max(ve)), '\\n')\n",
    "\n",
    "def errorRelativo_hess(f, Hessf, n: int , nt: int, h: float):\n",
    "    \"\"\"\n",
    "    Print statistics of relative errors in the Hessian matrix calculation (analytical vs. numerical)\n",
    "    using finite differences.\n",
    "    \n",
    "    Args:\n",
    "    - f:     function to find the Hessian.\n",
    "    - Hessf: analytical Hessian of f.\n",
    "    - n:     dimension.\n",
    "    - nt:    number of tests.\n",
    "    - h:     step size.\n",
    "    \"\"\"\n",
    "    ve = np.zeros(nt)\n",
    "    Hf = lambda x: HESSIAN(f = f, x = x, h = h) # Funcion Hessiana generada con autograd.\n",
    "    for i in range(nt):\n",
    "        x0  = np.random.randn(n)\n",
    "        H0  = Hessf(x0)\n",
    "        Ha  = Hf(x0)\n",
    "        ve[i] = np.linalg.norm(H0-Ha) / np.linalg.norm(Ha)\n",
    "    print('ERRORES RELATIVOS EN EL CÁLCULO DE LA HESSIANA PARA h =', h)\n",
    "    print('Min: %.2e   Media: %.2e    Max: %.2e' %(np.min(ve), np.mean(ve), np.max(ve)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNIDAD 8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS_MOD(f, gradf, xk: np.array, tol: float, Hk: np.array,\n",
    "             N: int, alpha_i, p: float, c: float, Nb: int):\n",
    "    \"\"\"\n",
    "    BFGS METHOD WITH MODIFICATION FOR THE HESSIAN MATRIX.\n",
    "\n",
    "    Args:\n",
    "    - f:       function to minimize.\n",
    "    - gradf:   gradient of the function.\n",
    "    - xk:      initial point.\n",
    "    - tol:     tolerance.\n",
    "    - Hk:      initial Hessian matrix.\n",
    "    - N:       maximum number of iterations.\n",
    "    - alpha_i: initial step size.\n",
    "    - p:       reduction factor for the step size.\n",
    "    - c:       constant for the Armijo condition.\n",
    "    - Nb:      maximum number of iterations for the backtracking line search.\n",
    "\n",
    "    Returns:\n",
    "    - xk:  optimal point.\n",
    "    - gk:  gradient at the optimal point.\n",
    "    - k:   number of iterations.\n",
    "    - T/F: if the method converged.\n",
    "    \"\"\"\n",
    "    n = len(xk)\n",
    "    for k in range(N-1):\n",
    "        gk = gradf(xk)\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            return xk, gk, k, True\n",
    "        pk = - Hk @ gk\n",
    "        if pk.T @ gk > 0:\n",
    "            lb1 = 10**(-5) + (pk.T @ gk)/(gk.T @ gk)\n",
    "            Hk = Hk + lb1*np.eye(n)\n",
    "            pk = pk - lb1*gk\n",
    "        ak = BACKTRAKING(alpha_i = alpha_i, p = p, c = c, xk = xk, f = f,\n",
    "                        fxk = f(xk), gradfxk = gk, pk = pk, Nb = Nb)[0]\n",
    "        xk_n = xk + ak * pk\n",
    "        gk_n = gradf(xk_n)\n",
    "        sk = xk_n - xk\n",
    "        yk = gk_n - gk\n",
    "        if yk.T @ sk <= 0:\n",
    "            lb2 = 10**(-5) - (yk.T @ sk)/(yk.T @ yk)\n",
    "            Hk = Hk + lb2*np.eye(n)\n",
    "        else:\n",
    "            rhok = 1/(yk.T @ sk)\n",
    "            Hk = (np.eye(n) - rhok*np.outer(sk,yk)) @ Hk @ (np.eye(n) - rhok*np.outer(yk,sk)) + rhok*np.outer(sk,sk)\n",
    "        xk = xk_n\n",
    "    return xk, gk, N, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_BFGS(f, gradf, xk: np.array, tol: float, Hk: np.array, m: int, max_iter: int):\n",
    "    \"\"\"\n",
    "    L-BFGS METHOD.\n",
    "    Args:\n",
    "    - f:     function to minimize.\n",
    "    - gradf: gradient of the function.\n",
    "    - xk:    initial point.\n",
    "    - tol:   tolerance.\n",
    "    - Hk:    initial Hessian matrix.\n",
    "    Outputs:\n",
    "    -\n",
    "    \"\"\"\n",
    "    gk = gradf(xk)\n",
    "    for i in range(max_iter):\n",
    "        pk = - Hk @ gk\n",
    "        ak = 1\n",
    "        xk_n = xk + ak * pk\n",
    "        if i > m:\n",
    "            break\n",
    "        sk = xk_n - xk\n",
    "        yk = gradf(xk_n) - gk\n",
    "    #k = 0\n",
    "    #while np.linalg.norm(gk) > tol:\n",
    "    #    pk = - Hk @ gk\n",
    "    #    ak = 1\n",
    "    #    xk_n = xk + ak * pk\n",
    "    #    if k > m:\n",
    "    #        break\n",
    "    #    sk = xk_n - xk\n",
    "    #    yk = gradf(xk_n) - gk\n",
    "    #    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNIDAD 9**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAUSS_NEWTON_METHOD(f, R, J, xk: np.array, N: int,\n",
    "                        tol: float, p: float, c: float, Nb: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Gauss-Newton method.\n",
    "\n",
    "    Args:\n",
    "    - f:   function to minimize.\n",
    "    - R:   residual function.\n",
    "    - J:   Jacobian matrix of R.\n",
    "    - xk:  initial guess.\n",
    "    - N:   maximum number of iterations.\n",
    "    - tol: tolerance.\n",
    "    - p:   reduction factor for the step size.\n",
    "    - c:   constant for the Armijo condition.\n",
    "    - Nb:  maximum number of iterations for the backtracking line search.\n",
    "\n",
    "    Returns:\n",
    "    - xk:  optimal point.\n",
    "    - pk:  optimal step.\n",
    "    - k:   number of iterations.\n",
    "    - T/F: if the algorithm converges.\n",
    "\n",
    "    \"\"\"\n",
    "    for k in range(N-1):\n",
    "        Rk = R(xk)\n",
    "        Jk = J(xk)\n",
    "        fk = f(xk)\n",
    "        gk = Jk.T @ Rk\n",
    "        pk = np.linalg.solve(Jk.T @ Jk, -gk)\n",
    "        if np.linalg.norm(pk) < tol:\n",
    "            return xk, pk, k, True\n",
    "        ak = BACKTRAKING(alpha_i = 1, p = p, c = c, xk = xk,\n",
    "                         f = f, fxk = fk, gradfxk = gk, pk = pk,\n",
    "                         Nb = Nb)[0]\n",
    "        xk = xk + ak * pk\n",
    "    return xk, pk, N, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNIDAD 10**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
